---
title: "Run Latent Class Growth Analyses and Growth Mixture Modelling for RAMP longitudinal abnalyses of common mental health symptoms: GAD and PHQ"
author: "K L Purves"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
    number_sections: false
    highlight: monochrome
    theme: cerulean
code_folding: hide

html_notebook:
  theme: cerulean
toc: yes
---

This workbook centralises analyses of the PHQ-9 and GAD-7 total scores longtiduinal analyses in the RAMP and COPING samples. This corresponds with analytic steps 1-2 in the pre-registration for project ["Moderators of symptom trajectories of depression and anxiety during the COVID-19 pandemic"](!https://osf.io/jvca5/).

The analytic steps that will be followed for PHQ and GAD respectively are:

1. perform latent growth curve analysis of total scores across all time points to identify the best fitting trajectory for the whole sample     
2. perform growth mixture modelling to identify latent classes in trajectory that may correspond with distinct subgroups in our data

***Setup***


```{r}
# clear global environment
remove(list = ls())
```


***note: some of the install and update processes require command line verification in the console. If you are running this script for the first time and it does not proceed, pen and check the console***


```{r setup, include=FALSE}

# Install / load packages

if(!require(knitr)){
  install.packages("knitr")
  library(knitr)
}

if(!require(summarytools)){
  install.packages("summarytools")
  library(summarytools)
}

if(!require(data.table)){
  install.packages("data.table")
  library(data.table)
}


if(!require(kableExtra)){
  install.packages("kableExtra")
  library(kableExtra)
}

if(!require(glue)){
  install.packages("glue")
  library(glue)
}

if(!require(MplusAutomation)){
  install.packages("MplusAutomation")
  library(MplusAutomation)
}



if(!require(texreg)){
  install.packages("texreg")
  library(texreg)
}

if(!require(relimp)){
  install.packages("relimp")
  library(relimp)
}

if(!require(BiocManager)){
  install.packages("BiocManager")
  library(BiocManager)
}

if(!require(rhdf5)){
  BiocManager::install("rhdf5")
  library(rhdf5)
}

if(!require(plyr)){
  install.packages("plyr")
  library(plyr)
}


if(!require(tidyverse)){
  install.packages("tidyverse")
  library(tidyverse)
}



## global workbook settings


knitr::opts_chunk$set(results = 'asis',      
                      comment = NA,
                      prompt = FALSE,
                      cache = FALSE, 
                      warning = FALSE,
                      echo = TRUE,
                      fig.height = 5,
                      fig.width = 10,
                      fig.align = "center")

st_options(plain.ascii = FALSE,       
            style = "rmarkdown",      
            footnote = NA,            
            subtitle.emphasis = FALSE)                         


options(scipen = 999)
```


***Colour palette***

RAMP colours (first) from website and second is a generated left tetradic (rectangular) colour complement of the RAMP logo green for contrast and complement. 
```{r ieso colour palette, include=FALSE}

pres.palette <- c("#4ED1B7", "#F6C66A", "#6A6680", "#1A3E72", "#FFED04","#3F9D8A", "#FFFFFF") 

prev.pres.palette<-c("#4590B8", "#4D68D1", "#D14D68", "#D1B74D", "#68D14D", "#B74DD1", "#D1754D")

```


# Latent Growth Curves 

Identify the best fitting latent growth curve form, i.e. that which fits all of the data best, on average, for each outcome.

Note that the MPlus inout time interval specifications go from increasing by 1 for the first  waves (we measured with two week itnervals) to increasing by 2 when we moved to monthly. The choice of interval is arbitrary, but the relative difference should be retained. 

## PHQ9: Current Depression Symptoms Latent Growth Curve Model {.tabset}

### overview of model alterations and adaptations

After manual checks of each stage, a number of alterations were needed. These are detailed here.

**low covatiance coverage**
The covariance coverage between some time points is lower than MPlus default. This prevents algortihm for missing data being applied (MLR) and standard errors cannot be computed, even when setting the coverage expectations to zero. 

Looking at our data structure and MPlus output, I suspect this is due to missing data from COPING survey for waves 2 & 3, and from the RAMP survey at wave 17 at present (due to data extraction issues which mean we do not have a recent RAMP extraction)    

Will try the following to resolve this:   

* Run without timepoints 2,3 and 17   
* This will require dropping the 4 piece trajectory model (which would only have one timepoint in the final piece) and only running the three piece version.    


In addition, I adapted the minimum coverage requirement. Default in MPlus is 0.1 Covariance coverage in MPlus refers to the amount of people who have non missing values on the pair of values. Thus, sensible minimum is sample size dependent, as there may still be a significant proportion of people who have non missing data even if that represents less than 10% of the sample in a large data set.

Our lowest remaining value is 0.002 between time point 3 and timepoint 13. The lowest sample size between these is 19298 at timepoint 13. 

```{r calculate number of people with non missing}

19298*0.002

```

~40 people have data on both time points. This is low, but given LCGA (with class division) can be performed on models with as few as 30 subjects [See MPlus message board, discussion with Linda Muthen](http://www.statmodel.com/discussion/messages/23/12750.html?1592615934), this seems acceptable.

* Will drop coverage requirement to 0.0017 as this correspinds to ~30 people from our time point with the least data (18314).   


### Data summary

```{r read in phq and check missing all cases}

phq_check <-  readRDS("../../../data_clean/phq/phq.clean_merged_total_scores.rds")

# get a list of the total score columns
total_cols <- grep("total_Wave", names(phq_check))

#count how many columns we have
n_cols <- length(total_cols)

# create a list of how many NAs are missing across these 18 columns per row
list_n_missing <- apply(phq_check[total_cols],1,function(x) sum(is.na(x)))

# count how many people are missing data for all columns (NA = total_cols)
list_all_missing <-  sum(list_n_missing ==n_cols)

print(dim(phq_check)[1])
print(list_all_missing)

```


Important note: all numbers correspond exactly with MPlus output numbers (n per wave and n missing all observations etc).
  
   
### Summary of models run

* linear
* quadratic
* piecewise version 1: four pieces, corresponding with locdowns and easing of restrictions between April 2020 and April 2021
* piecewise version 2: three pieces, corresponding with lockdowns and easing of restrictions, but with no inflection for the March 2021 onward restriction easing as these were gradual and not as temporally well defined. 

*all of the above, but with pairwise correlations between timepoints

#### Piecewise trajectory key dates

##### trajectory 1
1. Easing of restrictions after first lockdown (23 June 2020, first wave of data collected after this is wave 6, which began on June 30th) 

  Thus piece one will be baseline - wave 5 
  capturing mental health during the first lockdown, with easing of restrictions as a potential point of change
  
2. Christmas Lockdown and lockdown 3 in England (December 20, 2020 christmas lockdown, legal third lockdown began on the 6th of January. First wave of data after the christmas shut down and new national lockdown was wave 14, which began on 19 Jan. Wave 13 began on 15 December, and most responses likely recorded before short christmas lockdown was announced)

  Thus piece two will be wave 6  - wave 13
  capturing the summer period where restrictions were relaxed and heading toward the worsening situation of December. Decmeber and January restrictions a potnetil point for change

3. Ease of restrictions begins (8 march 2021, slowly released over March. Follow up 16 began 16 March, thus captures period shortly after and into the easing)
  Thus piece 3 will be wave 14 to wave 15   
  Capturing the time during the third lockdown with stay at home orders and restrictions renewed
  Easing of restrictions a potential inflection point. 
  

Piecewise therefore: 

1. Baseline - 5
2. 6 - 13
3. 14 - 15
4. 16 - 17

##### trajectory 2

Possible that this is too compacted and had too few measurement points in shorter lockdowns to be meaningful. Thus, will also test a smaller peicewise model capturing inflections for initial lockdown, easing of restrictions, renewed lockdown into April. Arguably gradual easing of restrictionsover the course of March - July does not qualify for a probably inflection point in MH trajectories. 


Piecewise2 therefore: 

1. Baseline - 5
2. 6 - 13
3. 14 - 17

#### run all models 

dont rerun models if there is already output for it (to save computational time), do show the output in the console

```{r run depression linear models}

runModels("./MPlus_input_lcga/PHQ", 
          recursive = FALSE,
          showOutput=TRUE,
          replaceOutfile="never")

```

### Examine model output {.tabset}
this reads in all output, including the gh5 plot files
```{r read depression model}

dep.traj.all <- readModels("./MPlus_input_lcga/PHQ", 
                           recursive=FALSE)

```


#### Check key indicators of model stability {.tabset}

- warnings  
- that none of the residual variances are negative and significant  
- loadings of indicators and intercept should all be 1, and estimates should relate to the time points you specified (e.g. lin 0, 1, 2; quad 0, 1, 4) 
- intercept 0 for all and correlations look sensible

##### warnings

at the moment this doesnt say which model the warning is from, so any warning requires checking to identify its source. Though order in list will give some indication
```{r check warnings}

justWarnings <- do.call("paste",
  sapply(dep.traj.all ,"[", "warnings"))

justWarnings

```

Warnings are all about missing data (missing on all obs, this number lines up with our expectations from the dataset in the section above) and the length of a comment n piecewise scripts


Otherwise, no warnings. Check one complete.

Note that there are more people missing on all observations having dropped timepoints 2,3, and 17. Was 7866, but this version is 8217 .


##### errors

Check any errors with running any of the models. Oytput here will not say whicu model it belongs to, so will need to manually examine if errors occu,

```{r check errors}

justErrors <- do.call("paste",
  sapply(dep.traj.all ,"[", "errors"))

justErrors

```

###### Issue 1: Models fail to finish running and converge (resolved)

Some of the models have failed to converge:    

* Piecwise with pairwise residual correlations    
* Quadratic   
* Quadratic with pariwise residual correlations


Steps:    
* Increased number of iterations to 5000 (Mplus default is 1000).    
* This worked somwhat for the piecewise and quadratic (without pairwise correlations) which now have more informative errors    
* This did not work at all for quadratic (with pairwise residual correlation). Increased iterations to 10000 for this one.    
* This worked.    


###### Issue 2: Models are no specified due to time point 3 parameter (resolved)

Now all three run, but could not compute standard errors as the models were misidentified or did not converge.

Issues are all with specified parameters now:

***quadratic***: Problem involving parameter 2 (T_3)
*   examining the data, I have made an error with the dataset. There are 4325 in this time point. This corresponds to the second follow up that is meant to be dropped from this datafile. Interestingly, T4 onwards is correct. T3 should have 2662 observations. Appears I have  wrongly dropped this wave instead of follow up 2. *Note: this is exactly what I had done. Corrected and rerun all models*

###### Issue 3: Models are no specified due to low coverage covariacne between t 3 and timepoint 6,8,16

Checked this manually, and there are only 4 people who did both timepoint 3 and timepoint 6, 4 who did timepoint 3 and tomepoint 8, and only 3 who did time point 3 and timepoint 16




##### check if any residual variances are negative and significant

###### Linear 
```{r check if residual variances negative lin}

model <- dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_linear.out$parameters$unstandardized
model[model$paramHeader == "Residual.Variances",]

```

###### Linear with pairwise correlations
```{r check if residual variances negative lin pairwise}

model <- dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_linear_withpairwiseCorrelations.out$parameters$unstandardized
model[model$paramHeader == "Residual.Variances",]

```

###### Quadratic 
```{r check if residual variances negative quad}

model <- dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_quadratic.out$parameters$unstandardized
model[model$paramHeader == "Residual.Variances",]

```

###### Quadratic with pairwise correlations
```{r check if residual variances negative quad pairwise}

model <- dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_quadratic_withPairwiseCorrelations.out$parameters$unstandardized
model[model$paramHeader == "Residual.Variances",]

```

###### Piecewise 1 (IGNORE)
```{r check if residual variances negative piece 1, eval=FALSE}

# model <- dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version1_4pieces.out$parameters$unstandardized
# model[model$paramHeader == "Residual.Variances",]

```

###### Piecewise 1 with pairwise correlations (IGNORE)
```{r check if residual variances negative piece 1 pairwise, eval=FALSE}

# model <- dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version1_4pieces_wPairwise_correlations.out$parameters$unstandardized
# model[model$paramHeader == "Residual.Variances",]

```


###### Piecewise 2
```{r check if residual variances negative piece 2}

model <- dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version2_3pieces.out$parameters$unstandardized
model[model$paramHeader == "Residual.Variances",]

```

###### Piecewise 2 with pairwise correlations
```{r check if residual variances negative piece 2 pairwise}

model <- dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version2_3pieces_wPairwiseCorrelations.out$parameters$unstandardized
model[model$paramHeader == "Residual.Variances",]

```

#### Detailed output for each model {.tabset}

***Check:***    
1. loadings of indicators and intercepts are 1    
2. Estimates relate to specified timepoints   
3. intercept 0 for all     
4. correlations look sensible      

Print output file for each correlation for examination of these details and any other relevant information

##### Linear 
```{r check full output negative lin}

dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_linear.out$output


```

##### Linear with pairwise correlations
```{r check full output negative lin pairwise}

dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_linear_withpairwiseCorrelations.out$output


```

##### Quadratic 
```{r check full output negative quad}

dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_quadratic.out$output


```

##### Quadratic with pairwise correlations
```{r check full output negative quad pairwise}

dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_quadratic_withPairwiseCorrelations.out$output


```

##### Piecewise 1
```{r check full output negative piece 1}

dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version1_4pieces.out$output


```

##### Piecewise 1 with pairwise correlations
```{r check full output negative piece 1 pairwise}

dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version1_4pieces_wPairwise_correlations.out$output


```


##### Piecewise 2
```{r check full output negative piece 2}

dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version2_3pieces.out$output


```

##### Piecewise 2 with pairwise correlations
```{r check full output negative piece 2 pairwise}

dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version2_3pieces_wPairwiseCorrelations.out$output


```



### Summary table of all fit indices

**To do** : The mplusautomation showsummarytable uses X11 and outputs in a separate window - if want to create one inside rmarkdown will have to do manually 

```{r Fit Indices for Depression LGC, warnings = FALSE}

showSummaryTable(dep.traj.all, keepCols=c("Title", "Observations", "Parameters", "AIC", "BIC", "CFI", "TLI", "SRMR", "RMSEA_Estimate"), sortBy="BIC") #throws error but does print!

```
### Plot {.tabset}

I show the plots for all runs that worked. Output for all runs is available in the GMM folder on GitHub to review input and output data that generated these plots. The main analytic sections above will only refer to the final model run. 

#### Run 1: all time points
##### Format Data for Plots 

Plotting the estimated means for the trajectories with adjacent residuals correlated as these all had better fit than uncorrelated models

```{r Format Depression Growth Curve Data}

dep.traj.run.1 <- readModels("./MPlus_input_lcga/PHQ/Output_Run1_alltimepoints", 
                           recursive=TRUE)


dat_dep_run_1<-data.frame(matrix(nrow=18, ncol=3))

colnames(dat_dep_run_1)<-c("Linear", "Quadratic",  "Piecewise3")

#  "LinearCor", "QuadraticCor","Piecewise4", "Piecewise4Cor",, "Piecewise3Cor"

dat_dep_run_1$Linear <-dep.traj.run.1$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_linear.out$gh5$means_and_variances_data$y_estimated_means$values

#dat_dep_run_1$LinearCor<-dep.traj.all$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_linear_withpairwiseCorrelations.out$gh5$means_and_variances_data$y_estimated_means$values

dat_dep_run_1$Quadratic<-dep.traj.run.1$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_quadratic.out$gh5$means_and_variances_data$y_estimated_means$values

#dat_dep_run_1$QuadraticCor<-dep.traj.run.1$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_quadratic_withPairwiseCorrelations.out$gh5$means_and_variances_data$y_estimated_means$values

#dat_dep_run_1$Piecewise4<-dep.traj.run.1$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version1_4pieces.out$gh5$means_and_variances_data$y_estimated_means$values

#dat_dep_run_1$Piecewise4Cor<-dep.traj.run.1$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version1_4pieces_wPairwise_correlations.out$gh5$means_and_variances_data$y_estimated_means$values


dat_dep_run_1$Piecewise3<-dep.traj.run.1$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version2_3pieces.out$gh5$means_and_variances_data$y_estimated_means$values

#dat_dep_run_1$Piecewise3Cor<-dep.traj.run.1$..MPlus_input_lcga.PHQ.phq_latentGrowthCurve_piecwewise_version2_3pieces_wPairwiseCorrelations.out$gh5$means_and_variances_data$y_estimated_means$values


dat_dep_run_1$timepoint<-c(0:17) 

dat_dep_run_1<-dat_dep_run_1%>%pivot_longer(-timepoint, names_to = "Model", values_to = "score")%>%mutate(Model = factor(Model, levels = c("Linear","Quadratic","Piecewise3")))

```

##### Growth Curve Plot 

```{r Depression Growth Curve Forms}

growth_curves_run1 <- ggplot() +
  geom_line(data = dat_dep_run_1,
        aes(x = timepoint, y = score, colour = Model), size = 1) +
  geom_point(data = dat_dep,
        aes(x = timepoint, y = score, colour = Model, shape=Model), show.legend = F) +
  scale_color_manual(values = pres.palette[c(4, 1:3)]) +
  labs(x = "Follow up time point", y ="PHQ9 Score") +
  scale_y_continuous(expand = c(0,0), limits = c(6,10), breaks=seq(6, 10, 3)) + 
  scale_x_continuous(expand = c(0,0), limits = c(-0.5,18.5), breaks=seq(0, 18, by = 1)) +
  theme(panel.grid.major.y = element_line(size = 0.5,
                                        linetype = 'dashed',
                                        colour = "gray"),
        axis.text = element_text(colour="black", size = 14),
        axis.title = element_text(colour="black", size = 16),
        legend.key = element_blank(),
        legend.text = element_text(colour = "black", size = 11),
        panel.background = element_blank()) 

growth_curves_run1

```



