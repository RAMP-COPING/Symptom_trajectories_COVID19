---
title: "Clean PHQ data for all waves. Save a seperate RAMP baseline data for CFA in addition to all wave for GMM"
author: "K L Purves"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: false
    number_sections: false
    highlight: monochrome
    theme: cerulean
code_folding: show

html_notebook:
  theme: cerulean
toc: yes
---

# Setup

```{r}
# clear global environment
remove(list = ls())
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      comment=NA,
                      prompt=FALSE,
                      cache=FALSE)
```

Note: load tidyverse last!
```{r Load packages}
if(!require(summarytools)){
  install.packages("summarytools")
  library(summarytools)
}
if(!require(ggplot2)){
  install.packages("ggplot2")
  library(ggplot2)
}

if(!require(psych)){
  install.packages("psych")
  library(psych)
}

if(!require(tidyverse)){
  install.packages("tidyverse")
  library(tidyverse)
}

if(!require(kableExtra)){
  install.packages("kableExtra")
  library(kableExtra)
}

if(!require(multiplex)){
  install.packages("multiplex")
  library(multiplex)
}

if(!require(modeest)){
  install.packages("modeest")
  library(modeest)
}
```

```{r}
# read in the files (save to environment)

source("../../../Credentials/paths_anhedonia_analysis.R")
```

## read in all functions from function library

```{r call in function library functions}
# source all functions in the function library folder
files.sources = paste0("../functions/",list.files("../functions"))
sapply(files.sources, source)

```

# COPING data preprocessing {.tabset}

## read in and rename

### COPING Baseline
```{r}
# Read in the data

COPING_EDGI <- readRDS(file = paste0(ilovedata, "/data_raw/2021-05-20/coping_edgi/phq_coping_edgi.rds"))
COPING_GLAD <- readRDS(file = paste0(ilovedata, "/data_raw/2021-05-20/coping_glad/phq_coping_glad.rds"))
COPING_NBR <- readRDS(file = paste0(ilovedata, "/data_raw/2021-05-20/coping_nbr/phq9_coping_nbr.rds"))

names(COPING_EDGI)
names(COPING_GLAD)
names(COPING_NBR)

```

Select & rename relevant columns (will be a function at some point)
EDGI
```{r}

COPING_EDGI.id <- COPING_EDGI %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs
  select(-phq.little_interest_or_pleasure_in_doing_things.1,
         -phq.feeling_down_depressed_or_hopeless.1,
         -phq.staying_asleep_sleeping_trouble.1,
         -phq.feeling_tired_or_having_little_energy.1,
         -phq.poor_appetite_or_overeating.1,
         -phq.family_feeling_bad_failure.1,
         -phq.trouble_concentrating_newspaper_reading.1,
         -phq.moving_fidgety_opposite_slowly.1,
         -phq.dead_hurting_thoughts.1,
         -phq.care_difficult_home_things,
         -phq.pandemic_feelings_felt,
         -externalDataReference,
         ID = externalDataReference, # ID
         startDate,
         endDate,
         phq9.pandemic_little_interest_or_pleasure = phq9.little_interest_or_pleasure_in_doing_things,
         phq9.pandemic_down_depressed_hopeless = phq9.feeling_down_depressed_or_hopeless,
         phq9.pandemic_trouble_falling_or_staying_asleep = phq9.staying_asleep_sleeping_trouble,
         phq9.pandemic_feel_tired_little_energy = phq9.feeling_tired_or_having_little_energy,
         phq9.pandemic_poor_appetite_or_overeating = phq9.poor_appetite_or_overeating,
         phq9.pandemic_feeling_like_a_failure = phq9.feeling_bad_failure_family,
         phq9.pandemic_trouble_concentrating = phq9.trouble_concentrating_reading_newspaper,
         phq9.pandemic_fidgeting_or_moving_slowly = phq9.moving_fidgety_noticed_opposite,
         phq9.pandemic_thoughts_better_off_dead = phq9.dead_hurting_thoughts) %>%
  relocate(ID, .before=startDate)


# Inspect colnames
colnames(COPING_EDGI.id)

```

GLAD
```{r}

COPING_GLAD.id <- COPING_GLAD %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs
  select(-phq.little_interest_or_pleasure_in_doing_things.1,
         -phq.feeling_down_depressed_or_hopeless.1,
         -phq.staying_asleep_sleeping_trouble.1,
         -phq.feeling_tired_or_having_little_energy.1,
         -phq.poor_appetite_or_overeating.1,
         -phq.feeling_bad_failure_family.1,
         -phq.trouble_concentrating_newspaper_reading.1,
         -phq.moving_fidgety_opposite_slowly.1,
         -phq.dead_hurting_thoughts.1,
         -phq.problems_made_difficult_care,
         -phq.pandemic_feelings_felt,
         -externalDataReference,
         ID = externalDataReference, # ID
         startDate,
         endDate,
         phq9.pandemic_little_interest_or_pleasure = phq.little_interest_or_pleasure_in_doing_things,
         phq9.pandemic_down_depressed_hopeless = phq.feeling_down_depressed_or_hopeless,
         phq9.pandemic_trouble_falling_or_staying_asleep = phq.staying_asleep_sleeping_trouble,
         phq9.pandemic_feel_tired_little_energy = phq.feeling_tired_or_having_little_energy,
         phq9.pandemic_poor_appetite_or_overeating = phq.poor_appetite_or_overeating,
         phq9.pandemic_feeling_like_a_failure = phq.feeling_bad_failure_family,
         phq9.pandemic_trouble_concentrating = phq.trouble_concentrating_newspaper_reading,
         phq9.pandemic_fidgeting_or_moving_slowly = phq.moving_fidgety_opposite_slowly,
         phq9.pandemic_thoughts_better_off_dead = phq.dead_hurting_thoughts) %>%
  relocate(ID, .before=startDate)


# Inspect colnames
colnames(COPING_GLAD.id)

```
NBR
```{r}

COPING_NBR.id <- COPING_NBR %>% #new dataset with ID
  drop_na(subjectid) %>% # Drop NAs
  select(-phq.little_interest_or_pleasure_in_doing_things.1,
         -phq.feeling_down_depressed_or_hopeless.1,
         -phq.staying_asleep_sleeping_trouble.1,
         -phq.feeling_tired_or_having_little_energy.1,
         -phq.poor_appetite_or_overeating.1,
         -phq.family_feeling_bad_failure.1,
         -phq.trouble_concentrating_newspaper_reading.1,
         -phq.moving_fidgety_opposite_slowly.1,
         -phq.dead_hurting_thoughts.1,
         -phq.problems_made_care_difficult,
         -phq.pandemic_feelings_felt,
         -externalDataReference,
         ID = subjectid, # ID
         startDate,
         endDate,
         phq9.pandemic_little_interest_or_pleasure = phq9.little_interest_or_pleasure_in_doing_things,
         phq9.pandemic_down_depressed_hopeless = phq9.feeling_down_depressed_or_hopeless,
         phq9.pandemic_trouble_falling_or_staying_asleep = phq9.staying_asleep_sleeping_trouble,
         phq9.pandemic_feel_tired_little_energy = phq9.feeling_tired_or_having_little_energy,
         phq9.pandemic_poor_appetite_or_overeating = phq9.poor_appetite_or_overeating,
         phq9.pandemic_feeling_like_a_failure = phq9.feeling_bad_failure_family,
         phq9.pandemic_trouble_concentrating = phq9.trouble_concentrating_reading_newspaper,
         phq9.pandemic_fidgeting_or_moving_slowly = phq9.moving_fidgety_noticed_opposite,
         phq9.pandemic_thoughts_better_off_dead = phq9.dead_hurting_thoughts) %>%
  relocate(ID, .before=startDate)


# Inspect colnames
colnames(COPING_NBR.id)

```


```{r merge coping baseline samples}
# merge GLAD, EDGI and NBR

COPING_baseline.id <- left_join(COPING_NBR.id, COPING_GLAD.id)
COPING_baseline.id <- left_join(COPING_baseline.id, COPING_EDGI.id)
  
# Check column names
COPING_baseline.id %>%
  colnames()

# Check dimensions
COPING_baseline.id %>%
  dim()

# Look at top rows of the data frame
COPING_baseline.id %>% 
  head()
```

### COPING follow up A

```{r Read in COPING follow up A data}
# Read in the data
COPING_followupA <- readRDS(file = paste0(ilovedata, "/data_raw/2021-05-20/coping_followupa/phq_coping_followupa.rds"))

  # Check column names
COPING_followupA %>%
  colnames()

# Check dimensions
COPING_followupA %>%
  dim()

# Look at top rows of the data frame
COPING_followupA %>% 
  head()
```

```{r}


COPING_followupA.id <- COPING_followupA %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs
  add_column(sample = "COPING_followupA", .after = "externalDataReference") %>% #create new column 
  select(-phq.difficult_daily_life_issues,
         ID = externalDataReference, # ID
         sample,
         startDate,
         endDate,
         phq9.pandemic_little_interest_or_pleasure = phq.little_interest_or_pleasure_in_doing_things,
         phq9.pandemic_down_depressed_hopeless = phq.feeling_down_depressed_or_hopeless,
         phq9.pandemic_trouble_falling_or_staying_asleep = phq.staying_asleep_sleeping_trouble,
         phq9.pandemic_feel_tired_little_energy = phq.feeling_tired_or_having_little_energy,
         phq9.pandemic_poor_appetite_or_overeating = phq.poor_appetite_or_overeating,
         phq9.pandemic_feeling_like_a_failure = phq.failure_feeling_bad_family,
         phq9.pandemic_trouble_concentrating = phq.newspaper_reading_things_trouble,
         phq9.pandemic_moving_slowy = phq.slowly_speaking_moving_noticed,
         phq9.pandemic_fidgeting = phq.fidgety_opposite_moving_restless,
         phq9.pandemic_thoughts_better_off_dead = phq.dead_hurting_thoughts)

# Inspect colnames
colnames(COPING_followupA.id)

```
  
### COPING follow up B

```{r Read in COPING follow up B data}
# Read in the data
COPING_followupB <- readRDS(file = paste0(ilovedata, "/data_raw/2021-05-20/coping_followupb/phq_coping_followupb.rds"))

# Check column names
COPING_followupB %>%
  colnames()

# Check dimensions
COPING_followupB %>%
  dim()

# Look at top rows of the data frame
COPING_followupB %>% 
  head()
```

Select & rename relevant columns (will be a function at some point)
```{r}


COPING_followupB.id <- COPING_followupB %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs
  add_column(sample = "COPING_followupB", .after = "externalDataReference") %>% #create new column 
  select(-phq.difficult_daily_life_issues,
         ID = externalDataReference, # ID
         sample,
         startDate,
         endDate,
         phq9.pandemic_little_interest_or_pleasure = phq.little_interest_or_pleasure_in_doing_things,
         phq9.pandemic_down_depressed_hopeless = phq.feeling_down_depressed_or_hopeless,
         phq9.pandemic_trouble_falling_or_staying_asleep = phq.staying_asleep_sleeping_trouble,
         phq9.pandemic_feel_tired_little_energy = phq.feeling_tired_or_having_little_energy,
         phq9.pandemic_poor_appetite_or_overeating = phq.poor_appetite_or_overeating,
         phq9.pandemic_feeling_like_a_failure = phq.failure_family_feeling_bad,
         phq9.pandemic_trouble_concentrating = phq.trouble_concentrating_newspaper_reading,
         phq9.pandemic_moving_slowy = phq.slowly_noticed_speaking_moving,
         phq9.pandemic_fidgeting = phq.fidgety_opposite_moving_restless,
         phq9.pandemic_thoughts_better_off_dead = phq.dead_hurting_thoughts)


# Inspect colnames
colnames(COPING_followupB.id)

```

### COPING followup A ongoing
```{r}
# Read in the data
COPING_followupA_ong <- readRDS(file = paste0(ilovedata, "/data_raw/2021-05-20/coping_followupa_ongoing/phq_coping_followupa_ongoing.rds"))

# Check column names
COPING_followupA_ong %>%
  colnames()

# Check dimensions
COPING_followupA_ong %>%
  dim()

# Look at top rows of the data frame
COPING_followupA_ong %>% 
  head()
```

Select & rename relevant columns (will be a function at some point)
```{r}

COPING_followupA_ong.id <- COPING_followupA_ong %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs

  add_column(sample = "COPING_followupA_ong", .after = "externalDataReference") %>% #create new column 
  select(-phq.difficult_daily_life_issues,
         ID = externalDataReference, # ID
         sample,
         startDate,
         endDate,
         phq9.pandemic_little_interest_or_pleasure = phq.little_interest_or_pleasure_in_doing_things,
         phq9.pandemic_down_depressed_hopeless = phq.feeling_down_depressed_or_hopeless,
         phq9.pandemic_trouble_falling_or_staying_asleep = phq.staying_asleep_sleeping_trouble,
         phq9.pandemic_feel_tired_little_energy = phq.feeling_tired_or_having_little_energy,
         phq9.pandemic_poor_appetite_or_overeating = phq.poor_appetite_or_overeating,
         phq9.pandemic_feeling_like_a_failure = phq.failure_feeling_bad_family,
         phq9.pandemic_trouble_concentrating = phq.newspaper_reading_things_trouble,
         phq9.pandemic_moving_slowy = phq.slowly_speaking_moving_noticed,
         phq9.pandemic_fidgeting = phq.fidgety_opposite_moving_restless,
         phq9.pandemic_thoughts_better_off_dead = phq.dead_hurting_thoughts)


# Inspect colnames
colnames(COPING_followupA_ong.id)

```

# fix 10 item issue
COPING follow up PHQ coded one item (*Moving or speaking so slowly that other people could have noticed?
Or the opposite - being so fidgety or restless that you have been moving around a lot more than usual?*) into two items due to early mistake which meant that only the psychomotor retardation was being captured in early cases. I am resolving by taking the highest value for either item as the persons response

```{r fix multiple item issue in COPING follow up PHQ}

COPING_followupA.id <- COPING_followupA.id %>%
  mutate(phq9.pandemic_fidgeting_or_moving_slowly = case_when(!is.na(phq9.pandemic_moving_slowy) & is.na(phq9.pandemic_fidgeting) ~ .$phq9.pandemic_moving_slowy,
                                                              is.na(phq9.pandemic_moving_slowy) & !is.na(phq9.pandemic_fidgeting) ~ .$phq9.pandemic_fidgeting,
                                                              phq9.pandemic_moving_slowy == phq9.pandemic_fidgeting ~ .$phq9.pandemic_moving_slowy,
                                                              phq9.pandemic_moving_slowy > phq9.pandemic_fidgeting ~ .$phq9.pandemic_moving_slowy,
                                                              phq9.pandemic_moving_slowy < phq9.pandemic_fidgeting ~ .$phq9.pandemic_fidgeting)) %>%
  select(-phq9.pandemic_moving_slowy,
         -phq9.pandemic_fidgeting)




COPING_followupA_ong.id <- COPING_followupA_ong.id %>%
  mutate(phq9.pandemic_fidgeting_or_moving_slowly = case_when(!is.na(phq9.pandemic_moving_slowy) & is.na(phq9.pandemic_fidgeting) ~ .$phq9.pandemic_moving_slowy,
                                                              is.na(phq9.pandemic_moving_slowy) & !is.na(phq9.pandemic_fidgeting) ~ .$phq9.pandemic_fidgeting,
                                                              phq9.pandemic_moving_slowy == phq9.pandemic_fidgeting ~ .$phq9.pandemic_moving_slowy,
                                                              phq9.pandemic_moving_slowy > phq9.pandemic_fidgeting ~ .$phq9.pandemic_moving_slowy,
                                                              phq9.pandemic_moving_slowy < phq9.pandemic_fidgeting ~ .$phq9.pandemic_fidgeting)) %>%
  select(-phq9.pandemic_moving_slowy,
         -phq9.pandemic_fidgeting)


COPING_followupB.id <- COPING_followupB.id %>%
  mutate(phq9.pandemic_fidgeting_or_moving_slowly = case_when(!is.na(phq9.pandemic_moving_slowy) & is.na(phq9.pandemic_fidgeting) ~ .$phq9.pandemic_moving_slowy,
                                                              is.na(phq9.pandemic_moving_slowy) & !is.na(phq9.pandemic_fidgeting) ~ .$phq9.pandemic_fidgeting,
                                                              phq9.pandemic_moving_slowy == phq9.pandemic_fidgeting ~ .$phq9.pandemic_moving_slowy,
                                                              phq9.pandemic_moving_slowy > phq9.pandemic_fidgeting ~ .$phq9.pandemic_moving_slowy,
                                                              phq9.pandemic_moving_slowy < phq9.pandemic_fidgeting ~ .$phq9.pandemic_fidgeting)) %>%
  select(-phq9.pandemic_moving_slowy,
         -phq9.pandemic_fidgeting)




names(COPING_followupA_ong.id)
names(COPING_followupA.id)
names(COPING_followupB.id)




```

## Creating follow up time points

### Create waves within the FOLLOW UP DATA 
Note: dates from google sheet from Kirstin
Checked with Gerome - set start date as the date the survey was released, and keep window open until the DAY BEFORE the next survey was released

NOTE: Am changing wave indicators to be in order

For those who did an earlier wave during the time frame of a later wave, I include them in the later wave.

Include RAMP waves in ordering, even though COPING waves started later. So some earlier waves will only be RAMP


```{r}

# April/May (wave 1) - FOLLOW UP A
ramp_start1 <- as.POSIXct("2020-04-21")
ramp_end1 <-  as.POSIXct("2020-05-04")  

# May (wave 2) - FOLLOW UP B
ramp_start2 <- as.POSIXct("2020-05-05")
ramp_end2<-  as.POSIXct("2020-05-18")  


# MAY 2 (wave 3) - FOLLOW UP A
start1 <- as.POSIXct("2020-05-19")
end1 <-  as.POSIXct("2020-06-01")  

# JUNE 1 (wave 4) - FOLLOW UP B
start2 <- as.POSIXct("2020-06-02")
end2<-  as.POSIXct("2020-06-15")  

# JUNE 2 (wave 5) - FOLLOW UP A
start3 <- as.POSIXct("2020-06-16")
end3 <-  as.POSIXct("2020-06-29") 

# JULY 1 (wave 6) - FOLLOW UP B
start4 <- as.POSIXct("2020-06-30")
end4<-  as.POSIXct("2020-07-13") 

# JULY 2 (wave 7) - FOLLOW UP A
start5 <- as.POSIXct("2020-07-14")
end5 <-  as.POSIXct("2020-07-27") ## Ongoing starts from here?

# JULY - AUG (wave 8) - FOLLOW UP B
start6 <- as.POSIXct("2020-07-28") 
end6 <-  as.POSIXct("2020-08-24") 

# AUG - SEPT (wave 9) - FOLLOW UP A
start7 <- as.POSIXct("2020-08-25")
end7 <-  as.POSIXct("2020-09-21") 

# SEPT - OCT (wave 10) - FOLLOW UP B
start8 <- as.POSIXct("2020-09-22")
end8 <-  as.POSIXct("2020-10-19") 

# OCT - NOV (wave 11) - FOLLOW UP A
start9 <- as.POSIXct("2020-10-20")
end9 <-  as.POSIXct("2020-11-16") 

# NOV - DEC (wave 12) - FOLLOW UP B
start10 <- as.POSIXct("2020-11-17")
end10 <-  as.POSIXct("2020-12-14") 

# DEC - JAN (wave 13) - FOLLOW UP A
start11 <- as.POSIXct("2020-12-15")
end11 <-  as.POSIXct("2021-01-11") 

# JAN - FEB (wave 14) - FOLLOW UP B
start12 <- as.POSIXct("2021-01-12")
end12 <-  as.POSIXct("2021-02-08") 

# FEB - MARCH (wave 15) - FOLLOW UP A
start13 <- as.POSIXct("2021-02-09")
end13 <-  as.POSIXct("2021-03-08") 

# MARCH - APR (wave 16) - FOLLOW UP B
start14 <- as.POSIXct("2021-03-09")
end14 <-  as.POSIXct("2021-04-05") 

# APR - MAY (wave 17) - FOLLOW UP A
start15 <- as.POSIXct("2021-04-06")
end15 <-  as.POSIXct("2021-05-03")

# MAY - JUNE (wave 18) - FOLLOW UP B
start16 <- as.POSIXct("2021-05-04")
end16 <-  as.POSIXct("2021-05-31")

# JUNE - JULY (wave 19) - FOLLOW UP A
start17 <- as.POSIXct("2021-06-01")
end17 <-  as.POSIXct("2021-06-28")

# JUNE - JULY (wave 20) - FOLLOW UP B
start18 <- as.POSIXct("2021-06-29")
end18 <-  as.POSIXct("2021-07-26")

# JUNE - JULY (wave 21) - FOLLOW UP A
start19 <- as.POSIXct("2021-07-27")
end19 <-  as.POSIXct("2021-08-17")

#JM**do we have data beyond this?


COPING_followupA.id <- COPING_followupA.id %>%
  mutate(waveA = case_when(startDate >= start1 & startDate < end1 ~ ".Wave_03",
                           startDate >= start2 & startDate <= end2 ~ ".Wave_04",
                            startDate >= start3 & startDate <= end3 ~ ".Wave_03", #repeat
                           startDate >= start4 & startDate <= end4 ~ ".Wave_06",
                            startDate >= start5 & startDate <= end5 ~ ".Wave_07",
                           startDate >= start6 & startDate <= end6 ~ ".Wave_08",
                            startDate >= start7 & startDate <= end7 ~ ".Wave_09",
                           startDate >= start8 & startDate <= end8 ~ ".Wave_10",
                            startDate >= start9 & startDate <= end9 ~ ".Wave_11",
                           startDate >= start10 & startDate <= end10 ~ ".Wave_12",
                            startDate >= start11 & startDate <= end11 ~ ".Wave_13",
                           startDate >= start12 & startDate <= end12 ~ ".Wave_14",
                            startDate >= start13 & startDate <= end13 ~ ".Wave_15",
                              startDate >= start14 & startDate <= end14 ~ ".Wave_16",
                            startDate >= start15 & startDate <= end15 ~ ".Wave_17",
                            startDate >= start16 & startDate <= end16 ~ ".Wave_18",
                           startDate >= start17 & startDate <= end17 ~ ".Wave_19",
                           startDate >= start18 & startDate <= end18 ~ ".Wave_20",
                            startDate >= start19 & startDate <= end19 ~ ".Wave_21"))

COPING_followupA_ong.id <- COPING_followupA_ong.id %>%
  mutate(waveA = case_when(startDate >= start1 & startDate < end1 ~ ".Wave_03",
                           startDate >= start2 & startDate <= end2 ~ ".Wave_04",
                            startDate >= start3 & startDate <= end3 ~ ".Wave_05",
                           startDate >= start4 & startDate <= end4 ~ ".Wave_06",
                            startDate >= start5 & startDate < start6 ~ ".Wave_07",
                           startDate >= start6 & startDate <= end6 ~ ".Wave_08",
                            startDate >= start7 & startDate <= end7 ~ ".Wave_09",
                           startDate >= start8 & startDate <= end8 ~ ".Wave_10",
                            startDate >= start9 & startDate <= end9 ~ ".Wave_11",
                           startDate >= start10 & startDate <= end10 ~ ".Wave_12",
                            startDate >= start11 & startDate <= end11 ~ ".Wave_13",
                           startDate >= start12 & startDate <= end12 ~ ".Wave_14",
                            startDate >= start13 & startDate <= end13 ~ ".Wave_15",
                              startDate >= start14 & startDate <= end14 ~ ".Wave_16",
                            startDate >= start15 & startDate <= end15 ~ ".Wave_17",
                            startDate >= start16 & startDate <= end16 ~ ".Wave_18",
                           startDate >= start17 & startDate <= end17 ~ ".Wave_19",
                           startDate >= start18 & startDate <= end18 ~ ".Wave_20",
                            startDate >= start19 & startDate <= end19 ~ ".Wave_21"))

COPING_followupB.id <- COPING_followupB.id %>%
  mutate(waveB = case_when(startDate >= start1 & startDate < end1 ~ ".Wave_3A_in_B", # should be impossible to exist
                           startDate >= start2 & startDate <= end2 ~ ".Wave_04",
                            startDate >= start3 & startDate <= end3 ~ ".Wave_05",
                           startDate >= start4 & startDate < start5 ~ ".Wave_06",
                            startDate >= start5 & startDate <= end5 ~ ".Wave_07",
                           startDate >= start6 & startDate <= end6 ~ ".Wave_08",
                            startDate >= start7 & startDate <= end7 ~ ".Wave_09",
                           startDate >= start8 & startDate <= end8 ~ ".Wave_10",
                            startDate >= start9 & startDate <= end9 ~ ".Wave_11",
                           startDate >= start10 & startDate <= end10 ~ ".Wave_12",
                            startDate >= start11 & startDate <= end11 ~ ".Wave_13",
                           startDate >= start12 & startDate <= end12 ~ ".Wave_14",
                            startDate >= start13 & startDate <= end13 ~ ".Wave_15",
                              startDate >= start14 & startDate <= end14 ~ ".Wave_16",
                            startDate >= start15 & startDate <= end15 ~ ".Wave_17",
                            startDate >= start16 & startDate <= end16 ~ ".Wave_18",
                           startDate >= start17 & startDate <= end17 ~ ".Wave_19",
                           startDate >= start18 & startDate <= end18 ~ ".Wave_20",
                            startDate >= start19 & startDate <= end19 ~ ".Wave_21"))

```

### check for number of entries per wave

```{r check for number of entries per wave COPING}
COPING_followupA.id %>%
  group_by(waveA) %>%
  count()

COPING_followupA_ong.id %>%
  group_by(waveA) %>%
  count()

COPING_followupB.id %>%
  group_by(waveB) %>%
  count()
```

### Identifying duplicate IDs in a single wave
```{r Identifying duplicate IDs in a single wave COPING}

##Identify dup IDs in a single wave (follow up A)
COPING_followupA.id %>%
   group_by(waveA, ID) %>%
   summarize(N = n()) %>%
   filter(N > 1)


##Identify dup IDs in a single wave (follow up A ongoing)
COPING_followupA_ong.id %>%
   group_by(waveA, ID) %>%
   summarize(N = n()) %>%
   filter(N > 1)


##Identify dup IDs in a single wave (B) 
COPING_followupB.id %>%
   group_by(waveB, ID) %>%
   summarize(N = n()) %>%
   filter(N > 1)

##Identify dup IDs in baseline 
COPING_baseline.id %>%
   group_by(ID) %>%
   summarize(N = n()) %>%
   filter(N > 1)


```

**removing duplicates, reatining the latest possible data where there are repeats**

### Want the LATER data entry from people who answered twice within a single wave 
We want there to be unique IDs within each of the waves
```{r}
##FOLLOW UP A
##confirm number of rows in current dataset (= 22542)
nrow(COPING_followupA.id)

#Filter out duplicated IDs AND wave, keeping the LAST entry
COPING_followupA.id <- COPING_followupA.id %>% 
    group_by(ID) %>% 
    filter(!duplicated(waveA,
                      fromLast = TRUE)
           )

##confirm number of rows in dataset after filtering (should be 22485)
nrow(COPING_followupA.id)

##FOLLOW UP A ONGOING
##confirm number of rows in current dataset (= 22542)
nrow(COPING_followupA_ong.id)

#Filter out duplicated IDs AND wave, keeping the LAST entry
COPING_followupA_ong.id <- COPING_followupA_ong.id %>% 
    group_by(ID) %>% 
    filter(!duplicated(waveA,
                      fromLast = TRUE)
           )

##confirm number of rows in dataset after filtering (should be 22485)
nrow(COPING_followupA_ong.id)


##FOLLOW UP B
##confirm number of rows in current dataset 
nrow(COPING_followupB.id)

#Filter out duplicated IDs AND wave, keeping the LAST entry
COPING_followupB.id <- COPING_followupB.id %>% 
    group_by(ID) %>% 
    filter(!duplicated(waveB,
                      fromLast = TRUE)
           )

##confirm number of rows in dataset after filtering 
nrow(COPING_followupB.id)

##BASELINE
##confirm number of rows in current dataset 
nrow(COPING_baseline.id)

#Filter out duplicated IDs AND wave, keeping the LAST entry
COPING_baseline.id <- COPING_baseline.id %>% 
    group_by(ID) %>% 
    filter(!duplicated(ID,
                          fromLast = TRUE)
           ) %>%
  ungroup(ID)

##confirm number of rows in dataset after filtering 
nrow(COPING_baseline.id)

```

## Change COPING follow up A and B from long to wide format (we want this for the numeric ones so we can sum them)
```{r COPING follow up A and B long to wide format}

COPING_followupA.id.long <- COPING_followupA.id %>%
  group_by(waveA) %>%
  tidyr::pivot_wider(id_cols = ID, 
    names_from = waveA, 
                     values_from = c(phq9.pandemic_little_interest_or_pleasure, 
                                     phq9.pandemic_down_depressed_hopeless, 
                                     phq9.pandemic_trouble_falling_or_staying_asleep,
                                     phq9.pandemic_feel_tired_little_energy, 
                                     phq9.pandemic_poor_appetite_or_overeating,
                                     phq9.pandemic_feeling_like_a_failure,
                                     phq9.pandemic_trouble_concentrating,
                                     phq9.pandemic_fidgeting_or_moving_slowly,
                                     phq9.pandemic_thoughts_better_off_dead)) 

COPING_followupA_ong.id.long <- COPING_followupA_ong.id %>%
  group_by(waveA) %>%
  tidyr::pivot_wider(id_cols = ID, 
    names_from = waveA, 
                     values_from = c(phq9.pandemic_little_interest_or_pleasure, 
                                     phq9.pandemic_down_depressed_hopeless, 
                                     phq9.pandemic_trouble_falling_or_staying_asleep,
                                     phq9.pandemic_feel_tired_little_energy, 
                                     phq9.pandemic_poor_appetite_or_overeating,
                                     phq9.pandemic_feeling_like_a_failure,
                                     phq9.pandemic_trouble_concentrating,
                                     phq9.pandemic_fidgeting_or_moving_slowly,
                                     phq9.pandemic_thoughts_better_off_dead)) 


COPING_followupB.id.long <- COPING_followupB.id %>%
  group_by(waveB) %>%
  tidyr::pivot_wider(id_cols = ID, 
    names_from = waveB, 
                     values_from = c(phq9.pandemic_little_interest_or_pleasure, 
                                     phq9.pandemic_down_depressed_hopeless, 
                                     phq9.pandemic_trouble_falling_or_staying_asleep,
                                     phq9.pandemic_feel_tired_little_energy, 
                                     phq9.pandemic_poor_appetite_or_overeating,
                                     phq9.pandemic_feeling_like_a_failure,
                                     phq9.pandemic_trouble_concentrating,
                                     phq9.pandemic_fidgeting_or_moving_slowly,
                                     phq9.pandemic_thoughts_better_off_dead)) 
  

# Check
colnames(COPING_followupA.id.long)
colnames(COPING_followupA_ong.id.long)
colnames(COPING_followupB.id.long)


```
  
# Clean COPING data {.tabset}

## Set minimum and maximum values 
```{r Set minimum and maximum values}
phq.min.scale <- 0
phq.max.scale <- 3
```


## COPING baseline
add wave 0 to the end of all variable names to indicate baseline
```{r Add "_.Wave_0" to the end of all variables COPING baseline}
COPING_baseline.id <- COPING_baseline.id %>% 
  rename_with( ~ paste(.x, ".Wave_0", sep = "_"), starts_with("phq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(COPING_baseline.id)
```

```{r Add "_unc" to the end of all variables COPING baseline}
COPING_baseline.id <- COPING_baseline.id %>% 
  rename_with( ~ paste(.x, "unc", sep = "_"), starts_with("phq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(COPING_baseline.id)
```

```{r COPING baseline inspect the variables}
COPING_baseline.id %>% 
  freq(phq9.pandemic_down_depressed_hopeless_.Wave_0_unc)
```

### Make list of variables for cleaning
```{r Make list of variables for cleaning}
phq.items.baseline_unc <- COPING_baseline.id %>% 
  select(contains("_unc")) %>% 
  colnames()

phq.items.baseline_unc
```

### Recode implausuble values
```{r Recode implausible values COPING baseline}
# Recode and clean
COPING_baseline.id <- COPING_baseline.id %>%
   mutate(
     across(all_of(phq.items.baseline_unc),
            .fns = list(clean = ~case_when( # add "_clean" onto the variables you are cleaning
              . == -77 | . == -88 | . == -99 ~ ., # leave as is 
              . < phq.min.scale | . > phq.max.scale ~ -66, # recode as implausible value
              TRUE ~ . # leave as is
            )
            )
     )
   ) %>%
   rename_at(
     vars(contains( "_unc_clean")), # specify variables that now contain "_clean"
     list(~paste0(gsub("_unc_clean", "", .))) # remove the "_clean" now that they have been cleaned
   )

# Check
colnames(COPING_baseline.id)
```
### Make new list of cleaned variables
```{r}
phq.items.baseline_clean <- COPING_baseline.id %>% 
  select(!contains("_unc")) %>% # select the cleaned variables (i.e. they don't contain the phrase "_unc")
  select(!"ID") %>% # deselect the ID variable
  colnames()

phq.items.baseline_clean
```
### Check number of implausible values
```{r}
# Check for implausible values
phq_baseline_imp_n <- COPING_baseline.id %>% 
  select(all_of(phq.items.baseline_clean)) %>% 
  filter(. == -66) %>% 
  nrow()

# If statement
if (phq_baseline_imp_n == 0) {
  print(paste0("The number of implausible values in the coping follow up A phq variables is ", phq_baseline_imp_n, ". Can leave these variables as they are."))
} else {
  print(paste0("The number of implausible values in the coping follow up A phq variables is ", phq_baseline_imp_n, ". Please investigate."))
}
```

### Select only the clean variables and replocate ID to front

Note: keep ID in this data set
```{r}
COPING_baseline.id.clean <- COPING_baseline.id %>% 
  select(!contains("_unc")) 

colnames(COPING_baseline.id.clean)
```

## COPING follow up A

```{r Add "_unc" to the end of all variables COPING follow up A}
COPING_followupA.id.long <- COPING_followupA.id.long %>% 
  rename_with( ~ paste(.x, "unc", sep = "_"), starts_with("phq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(COPING_followupA.id.long)
```

```{r COPING follow up A inspect the variables}
COPING_followupA.id.long %>% 
  freq(phq9.pandemic_little_interest_or_pleasure_.Wave_03_unc)
```

### Make list of variables for cleaning
```{r Make list of variables for cleaning COPING A}
phq.items.followupA_unc <- COPING_followupA.id.long %>% 
  select(contains("_unc")) %>% 
  colnames()

phq.items.followupA_unc
```

### Recode implausuble values
```{r Recode implausible values COPING A}
# Recode and clean
COPING_followupA.id.long <- COPING_followupA.id.long %>%
   mutate(
     across(all_of(phq.items.followupA_unc),
            .fns = list(clean = ~case_when( # add "_clean" onto the variables you are cleaning
              . == -77 | . == -88 | . == -99 ~ ., # leave as is 
              . < phq.min.scale | . > phq.max.scale ~ -66, # recode as implausible value
              TRUE ~ . # leave as is
            )
            )
     )
   ) %>%
   rename_at(
     vars(contains( "_unc_clean")), # specify variables that now contain "_clean"
     list(~paste0(gsub("_unc_clean", "", .))) # remove the "_clean" now that they have been cleaned
   )

# Check
colnames(COPING_followupA.id.long)
```
### Make new list of cleaned variables
```{r}
phq.items.followupA_clean <- COPING_followupA.id.long %>% 
  select(!contains("_unc")) %>% # select the cleaned variables (i.e. they don't contain the phrase "_unc")
  select(!"ID") %>% # deselect the ID variable
  colnames()

phq.items.followupA_clean
```
### Check number of implausible values
```{r}
# Check for implausible values
phq_followupA_imp_n <- COPING_followupA.id.long %>% 
  select(all_of(phq.items.followupA_clean)) %>% 
  filter(. == -66) %>% 
  nrow()

# If statement
if (phq_followupA_imp_n == 0) {
  print(paste0("The number of implausible values in the coping follow up A phq variables is ", phq_followupA_imp_n, ". Can leave these variables as they are."))
} else {
  print(paste0("The number of implausible values in the coping follow up A phq variables is ", phq_followupA_imp_n, ". Please investigate."))
}
```

### Select only the clean variables
Note: keep ID in this data set
```{r}
COPING_followupA.id.long.clean <- COPING_followupA.id.long %>% 
  select(!contains("_unc")) 

colnames(COPING_followupA.id.long.clean)
```

## COPING follow up A ongoing

```{r Add "_unc" to the end of all variables COPING follow up A ongoing}
COPING_followupA_ong.id.long <- COPING_followupA_ong.id.long %>% 
  rename_with( ~ paste(.x, "unc", sep = "_"), starts_with("phq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(COPING_followupA_ong.id.long)
```

```{r COPING follow up A ongoing inspect the variables}
COPING_followupA_ong.id.long %>% 
  freq(phq9.pandemic_little_interest_or_pleasure_.Wave_05_unc)
```

### Make list of variables for cleaning
```{r Make list of variables for cleaning COPING A ongoing}
phq.items.followupA_ong_unc <- COPING_followupA_ong.id.long %>% 
  select(contains("_unc")) %>% 
  colnames()

phq.items.followupA_ong_unc
```

### Recode implausible values
```{r Recode implausible values COPING A ongoing}
# Recode and clean
COPING_followupA_ong.id.long <- COPING_followupA_ong.id.long %>%
   mutate(
     across(all_of(phq.items.followupA_ong_unc),
            .fns = list(clean = ~case_when( # add "_clean" onto the variables you are cleaning
              . == -77 | . == -88 | . == -99 ~ ., # leave as is 
              . < phq.min.scale | . > phq.max.scale ~ -66, # recode as implausible value
              TRUE ~ . # leave as is
            )
            )
     )
   ) %>%
   rename_at(
     vars(contains( "_unc_clean")), # specify variables that now contain "_clean"
     list(~paste0(gsub("_unc_clean", "", .))) # remove the "_clean" now that they have been cleaned
   )

# Check
colnames(COPING_followupA_ong.id.long)
```

### Make new list of cleaned variables
```{r}
phq.items.followupA_ong_clean <- COPING_followupA_ong.id.long %>% 
  select(!contains("_unc")) %>% # select the cleaned variables (i.e. they don't contain the phrase "_unc")
  select(!"ID") %>% # deselect the ID variable
  colnames()

phq.items.followupA_ong_clean
```

### Check number of implausible values
```{r}
# Check for implausible values
phq_followupA_ong_imp_n <- COPING_followupA_ong.id.long %>% 
  select(all_of(phq.items.followupA_ong_clean)) %>% 
  filter(. == -66) %>% 
  nrow()

# If statement
if (phq_followupA_ong_imp_n == 0) {
  print(paste0("The number of implausible values in the coping follow up A ongoing phq variables is ", phq_followupA_ong_imp_n, ". Can leave these variables as they are."))
} else {
  print(paste0("The number of implausible values in the coping follow up A ongoing phq variables is ", phq_followupA_ong_imp_n, ". Please investigate."))
}
```

### Select only the clean variables
Note: keep ID in this data set
```{r}
COPING_followupA_ong.id.long.clean <- COPING_followupA_ong.id.long %>% 
  select(!contains("_unc")) 

colnames(COPING_followupA_ong.id.long.clean)
```


## COPING follow up B

```{r Add "_unc" to the end of all variables COPING follow up B}
COPING_followupB.id.long <- COPING_followupB.id.long %>% 
  rename_with( ~ paste(.x, "unc", sep = "_"), starts_with("phq")) # Add suffix "unc" for all uncleaned data columns
```

### Make list of variables for cleaning
```{r Make list of variables for cleaning COPING B}
phq.items.followupB_unc <- COPING_followupB.id.long %>% 
  select(contains("_unc")) %>% 
  colnames()

phq.items.followupB_unc
```

### Recode implausible values
```{r Recode implausible valuesCOPING B}
# Recode and clean
COPING_followupB.id.long <- COPING_followupB.id.long %>%
   mutate(
     across(all_of(phq.items.followupB_unc),
            .fns = list(clean = ~case_when( # add "_clean" onto the variables you are cleaning
              . == -77 | . == -88 | . == -99 ~ ., # leave as is 
              . < phq.min.scale | . > phq.max.scale ~ -66, # recode as implausible value
              TRUE ~ . # leave as is
            )
            )
     )
   ) %>%
   rename_at(
     vars(contains( "_unc_clean")), # specify variables that now contain "_clean"
     list(~paste0(gsub("_unc_clean", "", .))) # remove the "_clean" now that they have been cleaned
   )

# Check
colnames(COPING_followupB.id.long)
```
### Make new list of cleaned variables
```{r}
phq.items.followupB_clean <- COPING_followupB.id.long %>% 
  select(!contains("_unc")) %>% 
  select(!"ID") %>% 
  colnames()

phq.items.followupB_clean
```
### Check number of implausible values
```{r}
# Check for implausible values
phq_followupB_imp_n <- COPING_followupB.id.long %>% 
  select(all_of(phq.items.followupB_clean)) %>% 
  filter(. == -66) %>% 
  nrow()

# If statement
if (phq_followupB_imp_n == 0) {
  print(paste0("The number of implausible values in the coping follow up B phq variables is ", phq_followupB_imp_n, ". Can leave these variables as they are."))
} else {
  print(paste0("The number of implausible values in the coping follow up B phq variables is ", phq_followupB_imp_n, ". Please investigate."))
}
```

### Select only the clean variables
```{r}
COPING_followupB.id.long.clean <- COPING_followupB.id.long %>% 
  select(!contains("_unc"))

colnames(COPING_followupB.id.long.clean)
```


# Merge COPING Baseline, follow up A and B surveys
Note: full join (i.e. to get all participants in all three data sets)
```{r Merge COPING follow up A and B surveys}
phq.merging <- list(COPING_baseline.id.clean,
                    COPING_followupA_ong.id.long.clean,
                       COPING_followupA.id.long.clean,
                       COPING_followupB.id.long.clean)


phq.COPING <- plyr::join_all(phq.merging,
#                    by = "ID", #if included, this stops columns with the same name from combined. Allowing to merge on all common names columns
                    type = "full")

# relocate ID to the beginning

phq.COPING <- phq.COPING %>%
  relocate(ID, .before = startDate)
  
  
  
# check
dim(COPING_followupA.id.long.clean)
dim(COPING_followupA_ong.id.long.clean)
dim(COPING_followupB.id.long.clean)
dim(COPING_baseline.id.clean)
dim(phq.COPING)

colnames(phq.COPING)


# reorder so that it is easy to see each wave in place.
phq.COPING.reordered <- phq.COPING %>% 
  select(order(names(phq.COPING)),
         -startDate,
         -endDate)

colnames(phq.COPING.reordered)

```
  
# RAMP data preprocessing {.tabset}
## RAMP baseline 
```{r}
# read in baseline data
RAMP_baseline = readRDS(paste0(ilovedata,"/data_raw/2021-02-18/ramp/phq9_ramp.rds"))

# Check column names
RAMP_baseline %>%
  colnames()

# Check dimensions
RAMP_baseline %>%
  dim()

# Look at top rows of the data frame
RAMP_baseline %>% 
  head()
```

```{r}
#Recode column names to harmonised variable names

#phq.x.0 - pre pandemic
#phq.x.1 - Baseline current
#PHQ.pandemic.change - item asking if this is different from before pandemic

RAMP_baseline.id <- RAMP_baseline %>% #new dataset with ID
  drop_na("Login ID") %>% # Drop NAs
 select(-phq.little_interest_or_pleasure_in_doing_things.1,
         -phq.feeling_down_depressed_or_hopeless.1,
         -phq.staying_asleep_sleeping_trouble.1,
         -phq.feeling_tired_or_having_little_energy.1,
         -phq.poor_appetite_or_overeating.1,
         -phq.feeling_bad_failure_family.1,
         -phq.trouble_concentrating_newspaper_reading.1,
         -phq.moving_fidgety_opposite_slowly.1,
         -phq.dead_hurting_thoughts.1,
         -phq.pandemic_feelings_felt,
        -phq.pandemic_feelings_felt.1,
         -externalDataReference,
         ID = "Login ID", # ID
         -startDate,
         -endDate,
         phq9.pandemic_little_interest_or_pleasure = phq.little_interest_or_pleasure_in_doing_things,
         phq9.pandemic_down_depressed_hopeless = phq.feeling_down_depressed_or_hopeless,
         phq9.pandemic_trouble_falling_or_staying_asleep = phq.staying_asleep_sleeping_trouble,
         phq9.pandemic_feel_tired_little_energy = phq.feeling_tired_or_having_little_energy,
         phq9.pandemic_poor_appetite_or_overeating = phq.poor_appetite_or_overeating,
         phq9.pandemic_feeling_like_a_failure = phq.feeling_bad_failure_family,
         phq9.pandemic_trouble_concentrating = phq.trouble_concentrating_newspaper_reading,
         phq9.pandemic_fidgeting_or_moving_slowly = phq.moving_fidgety_opposite_slowly,
         phq9.pandemic_thoughts_better_off_dead = phq.dead_hurting_thoughts) %>%
  relocate(ID, .before = phq9.pandemic_little_interest_or_pleasure)
         

# Inspect colnames
colnames(RAMP_baseline.id)


```


```{r}
#seen but not answered (-99 & -77 ) as NA
RAMP_baseline.id <- RAMP_baseline.id %>%
  mutate_at(vars(starts_with("phq")),
            funs(case_when(

             . == -77 ~ NA_real_,
             TRUE ~  .)))
  

```

## RAMP follow up A

```{r Read in RAMP follow up A data}
# Read in the data
RAMP_followupA <- readRDS(file = paste0(ilovedata, "/data_raw/2021-04-09/ramp_followupa/phq_ramp_followupa.rds"))

# Check column names
RAMP_followupA %>%
  colnames()

# Check dimensions
RAMP_followupA %>%
  dim()

# Look at top rows of the data frame
RAMP_followupA %>% 
  head()
```


```{r }

RAMP_followupA.id <- RAMP_followupA %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs
  add_column(sample = "RAMP_followupA", .after = "externalDataReference") %>% #create new column 
  select(
         ID = externalDataReference, # ID
         sample,
         startDate,
         endDate,
         phq9.pandemic_little_interest_or_pleasure = phq.little_interest_or_pleasure_in_doing_things,
         phq9.pandemic_down_depressed_hopeless = phq.feeling_down_depressed_or_hopeless,
         phq9.pandemic_trouble_falling_or_staying_asleep = phq.staying_asleep_sleeping_trouble,
         phq9.pandemic_feel_tired_little_energy = phq.feeling_tired_or_having_little_energy,
         phq9.pandemic_poor_appetite_or_overeating = phq.poor_appetite_or_overeating,
         phq9.pandemic_feeling_like_a_failure = phq.failure_feeling_bad_family,
         phq9.pandemic_trouble_concentrating = phq.newspaper_reading_things_trouble,
         phq9.pandemic_fidgeting_or_moving_slowly = phq.moving_fidgety_opposite_slowly,
         phq9.pandemic_thoughts_better_off_dead = phq.dead_hurting_thoughts)

# Inspect colnames
colnames(RAMP_followupA.id)

```
  

## RAMP follow up B

```{r Read in RAMP follow up B data}
# Read in the data
RAMP_followupB <- readRDS(file = paste0(ilovedata, "/data_raw/2021-04-09/ramp_followupb/phq_ramp_followupb.rds"))

# Check column names
RAMP_followupB %>%
  colnames()

# Check dimensions
RAMP_followupB %>%
  dim()

# Look at top rows of the data frame
RAMP_followupB %>% 
  head()
```

Select & rename relevant columns (will be a function at some point)
```{r}


RAMP_followupB.id <- RAMP_followupB %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs
  add_column(sample = "RAMP_followupB", .after = "externalDataReference") %>% #create new column 
  select(
         ID = externalDataReference, # ID
         sample,
         startDate,
         endDate,
         phq9.pandemic_little_interest_or_pleasure = phq.little_interest_or_pleasure_in_doing_things,
         phq9.pandemic_down_depressed_hopeless = phq.feeling_down_depressed_or_hopeless,
         phq9.pandemic_trouble_falling_or_staying_asleep = phq.staying_asleep_sleeping_trouble,
         phq9.pandemic_feel_tired_little_energy = phq.feeling_tired_or_having_little_energy,
         phq9.pandemic_poor_appetite_or_overeating = phq.poor_appetite_or_overeating,
         phq9.pandemic_feeling_like_a_failure = phq.failure_family_feeling_bad,
         phq9.pandemic_trouble_concentrating = phq.trouble_concentrating_newspaper_reading,
         phq9.pandemic_fidgeting_or_moving_slowly = phq.fidgety_opposite_slowly_speaking,
         phq9.pandemic_thoughts_better_off_dead = phq.dead_hurting_thoughts)


# Inspect colnames
colnames(RAMP_followupB.id)

```

## Creating follow up time points RAMP 

### Create waves within the FOLLOW UP DATA 
Note: dates from google sheet from Kirstin
Checked with Gerome - set start date as the date the survey was released, and keep window open until the DAY BEFORE the next survey was released

these are in order, incorporating everyone into wave according to date of completion as per the COPING above. 

```{r}

# April/May (wave 1) - FOLLOW UP A
ramp_start1 <- as.POSIXct("2020-04-21")
ramp_end1 <-  as.POSIXct("2020-05-04")  

# May (wave 2) - FOLLOW UP B
ramp_start2 <- as.POSIXct("2020-05-05")
ramp_end2<-  as.POSIXct("2020-05-18")  

# May/June (wave 3) - FOLLOW UP A
ramp_start3 <- as.POSIXct("2020-05-19")
ramp_end3 <-  as.POSIXct("2020-06-01") 

# June 1 (wave 4) - FOLLOW UP B
ramp_start4 <- as.POSIXct("2020-06-02")
ramp_end4<-  as.POSIXct("2020-06-15") 

# June 2 (wave 5) - FOLLOW UP A
ramp_start5 <- as.POSIXct("2020-06-16")
ramp_end5 <-  as.POSIXct("2020-06-29") 

# June/July (wave 6) - FOLLOW UP B
ramp_start6 <- as.POSIXct("2020-06-30") 
ramp_end6 <-  as.POSIXct("2020-07-13") 

# July 1 (wave 7) - FOLLOW UP A
ramp_start7 <- as.POSIXct("2020-07-14")
ramp_end7 <-  as.POSIXct("2020-07-27") 

# July/August (wave 8) - FOLLOW UP B
ramp_start8 <- as.POSIXct("2020-07-28")
ramp_end8 <-  as.POSIXct("2020-08-24") 

# August/September (wave 9) - FOLLOW UP A
ramp_start9 <- as.POSIXct("2020-08-25")
ramp_end9 <-  as.POSIXct("2020-09-21") 

# September/October (wave 10) - FOLLOW UP B
ramp_start10 <- as.POSIXct("2020-09-22")
ramp_end10 <-  as.POSIXct("2020-10-19") 

# October/November (wave 11) - FOLLOW UP A
ramp_start11 <- as.POSIXct("2020-10-20")
ramp_end11 <-  as.POSIXct("2020-11-16") 

# November/December (wave 12) - FOLLOW UP B
ramp_start12 <- as.POSIXct("2020-11-17")
ramp_end12 <-  as.POSIXct("2020-12-14") 

# December/January (wave 13) - FOLLOW UP A
ramp_start13 <- as.POSIXct("2020-12-15")
ramp_end13 <-  as.POSIXct("2021-01-18") 

# January/February (wave 14) - FOLLOW UP B
ramp_start14 <- as.POSIXct("2021-01-19")
ramp_end14 <-  as.POSIXct("2021-02-15") 

# February/March (wave 15) - FOLLOW UP A
ramp_start15 <- as.POSIXct("2021-02-16")
ramp_end15 <-  as.POSIXct("2021-03-15")

# March/April (wave 16) - FOLLOW UP B
ramp_start16 <- as.POSIXct("2021-03-16")
ramp_end16 <-  as.POSIXct("2021-04-19")

# April/May (wave 17) - FOLLOW UP A
ramp_start17 <- as.POSIXct("2021-04-20")
ramp_end17 <-  as.POSIXct("2021-05-10")

# May (wave 18) - FOLLOW UP B
ramp_start18 <- as.POSIXct("2021-05-11")
ramp_end18 <-  as.POSIXct("2021-05-31")

# June (wave 19) - FOLLOW UP A
ramp_start19 <- as.POSIXct("2021-06-01")
ramp_end19 <-  as.POSIXct("2021-06-28")


#JM**do we have data beyond this?
## KLP corrected the below to ensure 6 people who were NA (1 in wave A, 5 in wave B) are correctly allocated to the correct wave. easy check for dates for these wave NAs is to run the following after you run these chunks the first time: RAMP_followupA.id$endDate[is.na(RAMP_followupA.id$waveA)]

RAMP_followupA.id <- RAMP_followupA.id %>%
  mutate(waveA = case_when(startDate >= ramp_start1 & startDate < ramp_start2 ~ ".Wave_01",
                           startDate >= ramp_start2 & startDate <= ramp_end2 ~ ".Wave_02",
                            startDate >= ramp_start3 & startDate <= ramp_end3 ~ ".Wave_03",
                           startDate >= ramp_start4 & startDate <= ramp_end4 ~ ".Wave_04",
                            startDate >= ramp_start5 & startDate <= ramp_end5 ~ ".Wave_05",
                           startDate >= ramp_start6 & startDate <= ramp_end6 ~ ".Wave_06",
                            startDate >= ramp_start7 & startDate <= ramp_end7 ~ ".Wave_07",
                           startDate >= ramp_start8 & startDate <= ramp_end8 ~ ".Wave_08",
                            startDate >= ramp_start9 & startDate <= ramp_end9 ~ ".Wave_09",
                           startDate >= ramp_start10 & startDate <= ramp_end10 ~ ".Wave_10",
                            startDate >= ramp_start11 & startDate <= ramp_end11 ~ ".Wave_11",
                           startDate >= ramp_start12 & startDate <= ramp_end12 ~ ".Wave_12",
                            startDate >= ramp_start13 & startDate <= ramp_end13 ~ ".Wave_13",
                              startDate >= ramp_start14 & startDate <= ramp_end14 ~ ".Wave_14",
                            startDate >= ramp_start15 & startDate <= ramp_end15 ~ ".Wave_15",
                            startDate >= ramp_start16 & startDate <= ramp_end16 ~ ".Wave_16",
                           startDate >= ramp_start17 & startDate <= ramp_end17 ~ ".Wave_17",
                           startDate >= ramp_start18 & startDate <= ramp_end18 ~ ".Wave_18",
                            startDate >= ramp_start19 & startDate <= ramp_end19 ~ ".Wave_19")
         )

RAMP_followupB.id <- RAMP_followupB.id %>%
  mutate(waveB = case_when(startDate >= ramp_start1 & startDate < ramp_end1 ~ ".Wave_1A_in_B", # should be impossible to exist
                           startDate >= ramp_start2 & startDate < ramp_start3 ~ ".Wave_02",
                            startDate >= ramp_start3 & startDate <= ramp_end3 ~ ".Wave_03",
                           startDate >= ramp_start4 & startDate <= ramp_end4 ~ ".Wave_04",
                            startDate >= ramp_start5 & startDate <= ramp_end5 ~ ".Wave_05",
                           startDate >= ramp_start6 & startDate <= ramp_end6 ~ ".Wave_06",
                            startDate >= ramp_start7 & startDate <= ramp_end7 ~ ".Wave_07",
                           startDate >= ramp_start8 & startDate <= ramp_end8 ~ ".Wave_08",
                            startDate >= ramp_start9 & startDate <= ramp_end9 ~ ".Wave_09",
                           startDate >= ramp_start10 & startDate <= ramp_end10 ~ ".Wave_10",
                            startDate >= ramp_start11 & startDate <= ramp_end11 ~ ".Wave_11",
                           startDate >= ramp_start12 & startDate <= ramp_end12 ~ ".Wave_12",
                            startDate >= ramp_start13 & startDate <= ramp_end13 ~ ".Wave_13",
                              startDate >= ramp_start14 & startDate <= ramp_end14 ~ ".Wave_14",
                            startDate >= ramp_start15 & startDate <= ramp_end15 ~ ".Wave_15",
                            startDate >= ramp_start16 & startDate <= ramp_end16 ~ ".Wave_16",
                           startDate >= ramp_start17 & startDate <= ramp_end17 ~ ".Wave_17",
                           startDate >= ramp_start18 & startDate <= ramp_end18 ~ ".Wave_18",
                            startDate >= ramp_start19 & startDate <= ramp_end19 ~ ".Wave_19"))



```


### check for number of entries per wave

```{r check for number of entries per wave RAMP}

RAMP_followupA.id %>%
  group_by(waveA) %>%
  count()


RAMP_followupB.id %>%
  group_by(waveB) %>%
  count()
```

### Identifying duplicate IDs in a single wave
```{r Identifying duplicate IDs in a single wave RAMP}

##Identify dup IDs in a single wave (baseline)
RAMP_baseline.id %>%
   group_by(ID) %>%
   summarize(N = n()) %>%
   filter(N > 1)

##Identify dup IDs in a single wave (follow up A)
RAMP_followupA.id %>%
   group_by(waveA, ID) %>%
   summarize(N = n()) %>%
   filter(N > 1)

##Identify dup IDs in a single wave (B) 
RAMP_followupB.id %>%
   group_by(waveB, ID) %>%
   summarize(N = n()) %>%
   filter(N > 1)


```

**removing duplicates, retaining the latest possible data where there are repeats**

### Want the LATER data entry from people who answered twice within a single wave 
We want there to be unique IDs within each of the waves
```{r}

##baseline

##confirm number of rows in current dataset
nrow(RAMP_baseline.id)

#Filter out duplicated IDs AND wave, keeping the LAST entry
RAMP_baseline.id <- RAMP_baseline.id %>% 
    group_by(ID) %>% 
    filter(!duplicated(ID,
                       fromLast = TRUE)
          
           )  %>%
             ungroup()

##confirm number of rows in dataset after filtering 
nrow(RAMP_baseline.id)


##FOLLOW UP A
##confirm number of rows in current dataset
nrow(RAMP_followupA.id)

#Filter out duplicated IDs AND wave, keeping the LAST entry
RAMP_followupA.id <- RAMP_followupA.id %>% 
    group_by(ID) %>% 
    filter(!duplicated(waveA,
                      fromLast = TRUE)
           )

##confirm number of rows in dataset after filtering 
nrow(RAMP_followupA.id)

##FOLLOW UP B
##confirm number of rows in current dataset 
nrow(RAMP_followupB.id)

#Filter out duplicated IDs AND wave, keeping the LAST entry
RAMP_followupB.id <- RAMP_followupB.id %>% 
    group_by(ID) %>% 
    filter(!duplicated(waveB,
                      fromLast = TRUE)
           )

##confirm number of rows in dataset after filtering 
nrow(RAMP_followupB.id)
```

## Change RAMP follow up A and B from long to wide format (we want this for the numeric ones so we can sum them)
```{r RAMP follow up A and B long to wide format}

RAMP_followupA.id.long <- RAMP_followupA.id %>%
  group_by(waveA) %>%
  tidyr::pivot_wider(id_cols = ID, 
    names_from = waveA, 
                     values_from = c(phq9.pandemic_little_interest_or_pleasure, 
                                     phq9.pandemic_down_depressed_hopeless, 
                                     phq9.pandemic_trouble_falling_or_staying_asleep,
                                     phq9.pandemic_feel_tired_little_energy, 
                                     phq9.pandemic_poor_appetite_or_overeating,
                                     phq9.pandemic_feeling_like_a_failure,
                                     phq9.pandemic_trouble_concentrating,
                                     phq9.pandemic_fidgeting_or_moving_slowly,
                                     phq9.pandemic_thoughts_better_off_dead)) 


RAMP_followupB.id.long <- RAMP_followupB.id %>%
  group_by(waveB) %>%
  tidyr::pivot_wider(id_cols = ID, 
    names_from = waveB, 
                     values_from = c(phq9.pandemic_little_interest_or_pleasure, 
                                     phq9.pandemic_down_depressed_hopeless, 
                                     phq9.pandemic_trouble_falling_or_staying_asleep,
                                     phq9.pandemic_feel_tired_little_energy, 
                                     phq9.pandemic_poor_appetite_or_overeating,
                                     phq9.pandemic_feeling_like_a_failure,
                                     phq9.pandemic_trouble_concentrating,
                                     phq9.pandemic_fidgeting_or_moving_slowly,
                                     phq9.pandemic_thoughts_better_off_dead)) 
  

# Check
colnames(RAMP_followupA.id.long)
colnames(RAMP_followupB.id.long)


```



# clean RAMP data {.tabset}

## RAMP Baseline
add wave 0 to the end of all variable names to indicate baseline
```{r Add "_.Wave_0" to the end of all variables RAMP baseline}
RAMP_baseline.id <- RAMP_baseline.id %>% 
  rename_with( ~ paste(.x, ".Wave_0", sep = "_"), starts_with("phq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(RAMP_baseline.id)
```

```{r Add "_unc" to the end of all variables RAMP baseline}
RAMP_baseline.id <- RAMP_baseline.id %>% 
  rename_with( ~ paste(.x, "unc", sep = "_"), starts_with("phq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(RAMP_baseline.id)
```

```{r RAMP Baseline inspect the variables}
RAMP_baseline.id %>% 
  freq(phq9.pandemic_down_depressed_hopeless_.Wave_0_unc)
```

### Make list of variables for cleaning
```{r Make list of variables for cleaning RAMP baseline}
phq.items.baseline_unc <- RAMP_baseline.id %>% 
  select(contains("_unc")) %>% 
  colnames()

phq.items.baseline_unc
```

### Recode implausuble values
```{r Recode implausible values RAMP baseline}
# Recode and clean
RAMP_baseline.id <- RAMP_baseline.id %>%
   mutate(
     across(all_of(phq.items.baseline_unc),
            .fns = list(clean = ~case_when( # add "_clean" onto the variables you are cleaning
              . == -77 | . == -88 | . == -99 ~ ., # leave as is 
              . < phq.min.scale | . > phq.max.scale ~ -66, # recode as implausible value
              TRUE ~ . # leave as is
            )
            )
     )
   ) %>%
   rename_at(
     vars(contains( "_unc_clean")), # specify variables that now contain "_clean"
     list(~paste0(gsub("_unc_clean", "", .))) # remove the "_clean" now that they have been cleaned
   )

# Check
colnames(RAMP_baseline.id)
```
### Make new list of cleaned variables
```{r}
phq.items.baseline_clean <- RAMP_baseline.id %>% 
  select(!contains("_unc")) %>% # select the cleaned variables (i.e. they don't contain the phrase "_unc")
  select(!"ID") %>% # deselect the ID variable
  colnames()

phq.items.baseline_clean
```
### Check number of implausible values
```{r}
# Check for implausible values
phq_baseline_imp_n <- RAMP_baseline.id %>% 
  select(all_of(phq.items.baseline_clean)) %>% 
  filter(. == -66) %>% 
  nrow()

# If statement
if (phq_baseline_imp_n == 0) {
  print(paste0("The number of implausible values in the RAMP Baseline PHQ variables is ", phq_baseline_imp_n, ". Can leave these variables as they are."))
} else {
  print(paste0("The number of implausible values in the RAMP Baseline PHQ variables is ", phq_baseline_imp_n, ". Please investigate."))
}
```

### Select only the clean variables
Note: keep ID in this data set
```{r}
RAMP_baseline.id.clean <- RAMP_baseline.id %>% 
  select(!contains("_unc")) 

colnames(RAMP_baseline.id.clean)
```


## RAMP follow up A

```{r Add "_unc" to the end of all variablesRAMP follow up A}
RAMP_followupA.id.long <- RAMP_followupA.id.long %>% 
  rename_with( ~ paste(.x, "unc", sep = "_"), starts_with("phq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(RAMP_followupA.id.long)
```

```{r RAMP follow up A inspect the variables}
RAMP_followupA.id.long %>% 
  freq(phq9.pandemic_little_interest_or_pleasure_.Wave_01_unc)
```

### Make list of variables for cleaning
```{r Make list of variables for cleaningRAMP A}
phq.items.followupA_unc <- RAMP_followupA.id.long %>% 
  select(contains("_unc")) %>% 
  colnames()

phq.items.followupA_unc
```

### Recode implausuble values
```{r Recode implausible values RAMP A}
# Recode and clean
RAMP_followupA.id.long <- RAMP_followupA.id.long %>%
   mutate(
     across(all_of(phq.items.followupA_unc),
            .fns = list(clean = ~case_when( # add "_clean" onto the variables you are cleaning
              . == -77 | . == -88 | . == -99 ~ ., # leave as is 
              . < phq.min.scale | . > phq.max.scale ~ -66, # recode as implausible value
              TRUE ~ . # leave as is
            )
            )
     )
   ) %>%
   rename_at(
     vars(contains( "_unc_clean")), # specify variables that now contain "_clean"
     list(~paste0(gsub("_unc_clean", "", .))) # remove the "_clean" now that they have been cleaned
   )

# Check
colnames(RAMP_followupA.id.long)
```


### Make new list of cleaned variables
```{r}
phq.items.followupA_clean <- RAMP_followupA.id.long %>% 
  select(!contains("_unc")) %>% # select the cleaned variables (i.e. they don't contain the phrase "_unc")
  select(!"ID") %>% # deselect the ID variable
  colnames()

phq.items.followupA_clean
```
### Check number of implausible values
```{r}
# Check for implausible values
phq_followupA_imp_n <- RAMP_followupA.id.long %>% 
  select(all_of(phq.items.followupA_clean)) %>% 
  filter(. == -66) %>% 
  nrow()

# If statement
if (phq_followupA_imp_n == 0) {
  print(paste0("The number of implausible values in the RAMP follow up A PHQ variables is ", phq_followupA_imp_n, ". Can leave these variables as they are."))
} else {
  print(paste0("The number of implausible values in the RAMP follow up A PHQ variables is ", phq_followupA_imp_n, ". Please investigate."))
}
```

### Select only the clean variables
Note: keep ID in this data set
```{r}
RAMP_followupA.id.long.clean <- RAMP_followupA.id.long %>% 
  select(!contains("_unc")) 

colnames(RAMP_followupA.id.long.clean)
```

## RAMP follow up B

```{r Add "_unc" to the end of all variables RAMP follow up B}
RAMP_followupB.id.long <- RAMP_followupB.id.long %>% 
  rename_with( ~ paste(.x, "unc", sep = "_"), starts_with("phq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(RAMP_followupB.id.long)
```

```{r RAMP follow up B inspect the variables}
RAMP_followupB.id.long %>% 
  freq(phq9.pandemic_little_interest_or_pleasure_.Wave_02_unc)
```

### Make list of variables for cleaning
```{r Make list of variables for cleaning RAMP B}
phq.items.followupB_unc <- RAMP_followupB.id.long %>% 
  select(contains("_unc")) %>% 
  colnames()

phq.items.followupB_unc
```

### Recode implausuble values
```{r Recode implausible values RAMP B}
# Recode and clean
RAMP_followupB.id.long <- RAMP_followupB.id.long %>%
   mutate(
     across(all_of(phq.items.followupB_unc),
            .fns = list(clean = ~case_when( # add "_clean" onto the variables you are cleaning
              . == -77 | . == -88 | . == -99 ~ ., # leave as is 
              . < phq.min.scale | . > phq.max.scale ~ -66, # recode as implausible value
              TRUE ~ . # leave as is
            )
            )
     )
   ) %>%
   rename_at(
     vars(contains( "_unc_clean")), # specify variables that now contain "_clean"
     list(~paste0(gsub("_unc_clean", "", .))) # remove the "_clean" now that they have been cleaned
   )

# Check
colnames(RAMP_followupB.id.long)
```
### Make new list of cleaned variables
```{r}
phq.items.followupB_clean <- RAMP_followupB.id.long %>% 
  select(!contains("_unc")) %>% # select the cleaned variables (i.e. they don't contain the phrase "_unc")
  select(!"ID") %>% # deselect the ID variable
  colnames()

phq.items.followupB_clean
```
### Check number of implausible values
```{r}
# Check for implausible values
phq_followupB_imp_n <- RAMP_followupB.id.long %>% 
  select(all_of(phq.items.followupB_clean)) %>% 
  filter(. == -66) %>% 
  nrow()

# If statement
if (phq_followupB_imp_n == 0) {
  print(paste0("The number of implausible values in the RAMP follow up B PHQ variables is ", phq_followupB_imp_n, ". Can leave these variables as they are."))
} else {
  print(paste0("The number of implausible values in the RAMP follow up B PHQ variables is ", phq_followupB_imp_n, ". Please investigate."))
}
```

### Select only the clean variables
Note: keep ID in this data set
```{r}
RAMP_followupB.id.long.clean <- RAMP_followupB.id.long %>% 
  select(!contains("_unc")) 

colnames(RAMP_followupB.id.long.clean)
```

# Save baseline RAMP cleaned data items only for CFA
```{r}

saveRDS(object = RAMP_baseline.id.clean, file = paste0("../../../data_clean/phq/phq.clean_baseline_items.RAMP",  ".rds"))

```

# Merge RAMP baseline, follow up A and B surveys
Note: full join (i.e. to get all participants in all three data sets)
```{r Merge RAMP follow up A and B surveys}
phq.merging <- list(RAMP_baseline.id.clean,
                       RAMP_followupA.id.long.clean,
                       RAMP_followupB.id.long.clean)


phq.RAMP <- plyr::join_all(phq.merging,
     #               by = "ID", freeing to merge on all common columns so we dont drop anyything needlessly
                    type = "full")

  
# check
dim(RAMP_followupA.id.long.clean)
dim(RAMP_followupB.id.long.clean)
dim(RAMP_baseline.id.clean)
dim(phq.RAMP)

colnames(phq.RAMP)


# reorder so that it is easy to see each wave in place.
phq.RAMP.reordered <- phq.RAMP %>% 
  select(order(names(phq.RAMP))) 

colnames(phq.RAMP.reordered)

```

# merge COPING and RAMP all wave data

Merge by all common columns so that we dont duplicate waves. This is effectively an rbind action (adding a row for every case but keeping columns standard across)

```{r merge RAMP and COPING}

# add a sample columns
phq.RAMP.for.merging <- phq.RAMP.reordered %>%
  mutate(sample = "RAMP") %>%
  relocate(sample, .after = ID)

phq.COPING.for.merging <- phq.COPING.reordered %>%
  mutate(sample = "COPING") %>%
  relocate(sample, .after = ID)

phq.merged <- full_join(phq.RAMP.for.merging,phq.COPING.for.merging)

dim(phq.RAMP.for.merging)
dim(phq.COPING.for.merging)
dim(phq.merged)
names(phq.merged)
```

## drop data from waves after 6 April 2021 (see pre registration for re-agreed data boundaries)

```{r drop later waves}
phq.clean <- phq.merged %>%
  select(!contains("Wave_18"))

names(phq.clean)
```

# create sum scores
scoring variables to use in function below
```{r}
#Scoring variables
PHQ.n.items = 9 # Enter here the total number of items of the questionnaire
PHQ.maximum.missing.items = 2 # Enter here the number of required items a participant can miss before they are dropped

#Limits for data cleaning
PHQ.total.score_upper_limit = 27
PHQ.total.score_lower_limit = 0

PHQ.min.value = 0
PHQ.max.value = 3
```

### make -77 NA
This will mean we can no longer distinguish between missing variable and someone who is missing altogether(did not complete survey) but this should be evident from the number of peope whos NA == 7 per wave.

```{r recalculate - 77}
phq.clean<- na_if(phq.clean, -77)
```


## add columns for sum scores for each wave. {.tabset}
use functions that generate items for phq using wave number as a user input. This function is specific to my data naming conventions.

Use a second function to calculate the total scores. This can be applied to any scale. it takes a dataframe, list of keys, list of items, minimum and maximum item values, and maximum allowed missing items. It calculates total scores, using mean imputation for any missing items. It drops scores for anyone who misses more than the maximum allowed items. It describes how many missing items there are, and the total scores after dropping anyone for high missingness, and shows the internal consistency metrics for the scale. Finally, it returns  column of total scores as output.


***Will do a chunk for every wave to make output clear and easy to distinguish and examine for every wave***

### Baseline 
```{r baseline sumscore test}

keys_phq <- c(1,1,1,1,1,1,1,1,1)
items_phq <- generate_items_phq(0)

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_0 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 01
```{r}


items_phq <- generate_items_phq("01")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_01 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 02
```{r}


items_phq <- generate_items_phq("02")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_02 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 03
```{r}


items_phq <- generate_items_phq("03")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_03 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 04
```{r}


items_phq <- generate_items_phq("04")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_04 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 05
```{r}


items_phq <- generate_items_phq("05")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_05 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 06
```{r}


items_phq <- generate_items_phq("06")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_06 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 07
```{r}


items_phq <- generate_items_phq("07")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_07 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 08
```{r}


items_phq <- generate_items_phq("08")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_08 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 09
```{r}


items_phq <- generate_items_phq("09")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_09 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 10
```{r}


items_phq <- generate_items_phq("10")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_10 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 11
```{r}


items_phq <- generate_items_phq("11")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_11 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 12
```{r}


items_phq <- generate_items_phq("12")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_12 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 13
```{r}


items_phq <- generate_items_phq("13")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_13 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 14
```{r}


items_phq <- generate_items_phq("14")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_14 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 15
```{r}


items_phq <- generate_items_phq("15")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_15 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 16
```{r}


items_phq <- generate_items_phq("16")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_16 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

### Follow up 17
```{r}


items_phq <- generate_items_phq("17")

phq.clean <- phq.clean %>%
  mutate(phq.total_Wave_17 = calculate_totals(dataframe=.,
                                                itemlist = items_phq,
                                                keylist = keys_phq,
                                                minval = PHQ.min.value,
                                                maxval = PHQ.max.value,
                                                maxmissing = PHQ.maximum.missing.items))
```

# select total scores per wave and save this dataset

```{r drop item data}

phq.totals.clean <-
  phq.clean %>%
  select(ID, sample,
         starts_with("phq.total_Wave_"))
```

## save
```{r save final data}

saveRDS(object = phq.totals.clean, file = paste0("../../../data_clean/phq/phq.clean_merged_total_scores",  ".rds"))
saveRDS(object = phq.clean, file = paste0("../../../data_clean/phq/phq.clean_merged_items",  ".rds"))

```

## save as csv without any rown names for MPLUS

create a list of column names in order to save alongside the data file 
```{r mplus remove id make name codebook}

phq.mplus <- phq.totals.clean %>%
  select(-sample)

phq.mplus.names <- names(phq.mplus)
phq.mplus.names.columns <- seq(1,length(phq.mplus.names))

phq.mplus.names <- data.frame(cbind(phq.mplus.names,phq.mplus.names.columns))

names(phq.mplus.names) <- c("Variable","Column")

```
## remove column names
```{r remove col names}

names(phq.mplus) <- NULL

```
##save .dat file for MPlus without headers, and corresponding orderd list of variables

```{r save plus dat files}

write.dat(phq.mplus, "../../../data_clean/phq/phq.clean_merged_total_scores.dat")
write_csv(phq.mplus.names, "../../../data_clean/phq/phq.clean_merged_total_scores.dat/phq.clean_merged_total_scores_codebook.csv")

```

# Obtain mode for questionaire date for each wave, and the variance in start date for each wave to add to our summary tables

As per pre reg specification. Not I said I would provide the average start date, however on reflection this does not make sense and mode is a more informative metric. Thus this will be a minor point of deviation from the preregistration document.


## get mode, earliest and latest time points for all
```{r create table of mode and variance for dates}

RAMPbase <- RAMP_baseline %>%
    summarise(mode = mlv(format(as.POSIXct(startDate), format='%d/%m/%Y'),method='mfv'),
            earliest = min(as.Date(startDate,format = '%d/%m/%Y')),
            latest = max(as.Date(startDate,format = '%d/%m/%Y'))) %>%
    mutate(sample = "RAMP",
           mode = as.Date(mode,format='%d/%m/%Y'),
           time.point = "Baseline") %>%
  select(time.point,sample,mode,earliest,latest)


RAMPA <- RAMP_followupA.id %>%
  group_by(waveA) %>%
   summarise(mode = mlv(format(as.POSIXct(startDate), format='%d/%m/%Y'),method='mfv'),
            earliest = min(as.Date(startDate,format = '%d/%m/%Y')),
            latest = max(as.Date(startDate,format = '%d/%m/%Y'))) %>%
    mutate(sample = "RAMP",
           mode = as.Date(mode,format='%d/%m/%Y')) %>%
  rename(time.point = waveA) %>%
  select(time.point,sample,mode,earliest,latest)

RAMPB <- RAMP_followupB.id %>%
  group_by(waveB) %>%
  summarise(mode = mlv(format(as.POSIXct(startDate), format='%d/%m/%Y'),method='mfv'),
            earliest = min(as.Date(startDate,format = '%d/%m/%Y')),
            latest = max(as.Date(startDate,format = '%d/%m/%Y'))) %>%
    mutate(sample = "RAMP",
           mode = as.Date(mode,format='%d/%m/%Y')) %>%
  rename(time.point = waveB) %>%
  select(time.point,sample,mode,earliest,latest)

COPEbase <- COPING_baseline.id %>%
   summarise(mode = mlv(format(as.POSIXct(startDate), format='%d/%m/%Y'),method='mfv'),
            earliest = min(as.Date(startDate,format = '%d/%m/%Y')),
            latest = max(as.Date(startDate,format = '%d/%m/%Y'))) %>%
    mutate(sample = "COPING",
           mode = as.Date(mode,format='%d/%m/%Y'),
           time.point = "Baseline") %>%
  select(time.point,sample,mode,earliest,latest)


COPINGA <- COPING_followupA.id %>%
  group_by(waveA) %>%
  summarise(mode = mlv(format(as.POSIXct(startDate), format='%d/%m/%Y'),method='mfv'),
            earliest = min(as.Date(startDate,format = '%d/%m/%Y')),
            latest = max(as.Date(startDate,format = '%d/%m/%Y'))) %>%
    mutate(sample = "COPING",
           mode = as.Date(mode,format='%d/%m/%Y')) %>%
  rename(time.point = waveA) %>%
  select(time.point,sample,mode,earliest,latest)


COPINGAong <- COPING_followupA_ong.id %>%
  group_by(waveA) %>%
    summarise(mode = mlv(format(as.POSIXct(startDate), format='%d/%m/%Y'),method='mfv'),
            earliest = min(as.Date(startDate,format = '%d/%m/%Y')),
            latest = max(as.Date(startDate,format = '%d/%m/%Y'))) %>%
    mutate(sample = "COPING",
           mode = as.Date(mode,format='%d/%m/%Y')) %>%
  rename(time.point = waveA) %>%
  select(time.point,sample,mode,earliest,latest)

COPINGB <- COPING_followupB.id %>%
  group_by(waveB) %>%
  summarise(mode = mlv(format(as.POSIXct(startDate), format='%d/%m/%Y'),method='mfv'),
            earliest = min(as.Date(startDate,format = '%d/%m/%Y')),
            latest = max(as.Date(startDate,format = '%d/%m/%Y'))) %>%
    mutate(sample = "COPING",
           mode = as.Date(mode,format='%d/%m/%Y')) %>%
  rename(time.point = waveB) %>%
  select(time.point,sample,mode,earliest,latest)


# create combined data 

BaseSet <- full_join(COPING_baseline.id,RAMP_baseline) %>%
    summarise(mode = mlv(format(as.POSIXct(startDate), format='%d/%m/%Y'),method='mfv'),
            earliest = min(as.Date(startDate,format = '%d/%m/%Y')),
            latest = max(as.Date(startDate,format = '%d/%m/%Y'))) %>%
    mutate(sample = "Combined",
           mode = as.Date(mode,format='%d/%m/%Y'),
           time.point = "Baseline") %>%
  select(time.point,sample,mode,earliest,latest)

Aset <- full_join(COPING_followupA.id,RAMP_followupA.id) %>%
  group_by(waveA) %>%
   summarise(mode = mlv(format(as.POSIXct(startDate), format='%d/%m/%Y'),method='mfv'),
            earliest = min(as.Date(startDate,format = '%d/%m/%Y')),
            latest = max(as.Date(startDate,format = '%d/%m/%Y'))) %>%
    mutate(sample = "Combined",
           mode = as.Date(mode,format='%d/%m/%Y')) %>%
  rename(time.point = waveA) %>%
  select(time.point,sample,mode,earliest,latest)

Bset <- full_join(COPING_followupB.id,RAMP_followupB.id) %>%
  group_by(waveB) %>%
   summarise(mode = mlv(format(as.POSIXct(startDate), format='%d/%m/%Y'),method='mfv'),
            earliest = min(as.Date(startDate,format = '%d/%m/%Y')),
            latest = max(as.Date(startDate,format = '%d/%m/%Y'))) %>%
    mutate(sample = "Combined",
           mode = as.Date(mode,format='%d/%m/%Y')) %>%
  rename(time.point = waveB) %>%
  select(time.point,sample,mode,earliest,latest)

```


## merge together the summary tables
```{r merge time summary}

date.merging <- list(RAMPbase,
                     COPEbase,
                     RAMPA,
                     COPINGA,
                     COPINGAong,
                     RAMPB,
                     COPINGB,
                     BaseSet,
                     Aset,
                     Bset)

date.merge <- plyr::join_all(date.merging,
                             by=c("time.point","sample"),
                    type = "full") %>%
  filter(!time.point == ".Wave_18") %>%
  mutate(time.point = case_when(time.point == ".Wave_01"~ "Follow up 01",
                                time.point == ".Wave_02"~ "Follow up 02",
                                time.point == ".Wave_03"~ "Follow up 03",
                                time.point == ".Wave_04"~ "Follow up 04",
                                time.point == ".Wave_05"~ "Follow up 05",
                                time.point == ".Wave_06"~ "Follow up 06",
                                time.point == ".Wave_07"~ "Follow up 07",
                                time.point == ".Wave_08"~ "Follow up 08",
                                time.point == ".Wave_09"~ "Follow up 09",
                                time.point == ".Wave_10"~ "Follow up 10",
                                time.point == ".Wave_11"~ "Follow up 11",
                                time.point == ".Wave_12"~ "Follow up 12",
                                time.point == ".Wave_13"~ "Follow up 13",
                                time.point == ".Wave_14"~ "Follow up 14",
                                time.point == ".Wave_15"~ "Follow up 15",
                                time.point == ".Wave_16"~ "Follow up 16",
                                time.point == ".Wave_17"~ "Follow up 17",
                                TRUE ~"Baseline")) %>%
  arrange(time.point,sample)

# get the earliest date where there were duplicate waves due to COPING ongoing and coping (waves 6,7 and 8)

date.merge <- date.merge %>%
  group_by(time.point,sample) %>%
  summarise(most.frequent.completion.date =  min(mode),
            earliest.completion.date = min(earliest),
            latest.completion.date = max(latest))




# reformat dates

date.merge <- date.merge %>%
  mutate(most.frequent.completion.date = format(most.frequent.completion.date, "%d-%m-%Y"),
         earliest.completion.date = format(earliest.completion.date, "%d-%m-%Y"),
         latest.completion.date = format(latest.completion.date, "%d-%m-%Y"))

# check

colnames(date.merge)
print(date.merge)

```


# summary tables 

## create a  summary table for every wave

each component table by wave
```{r built summary table 0}

summary_table_0 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_0,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_0,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_0)))  %>%
    mutate(sample = "Combined",
      time.point = "Baseline") %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_0 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_0,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_0,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_0)))  %>%
    mutate(time.point = "Baseline" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_0 <- rbind (summary_table_sample_0,summary_table_0)

```



```{r built summary table 01}

summary_table_01 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_01,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_01,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_01)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 01" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_01 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_01,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_01,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_01)))  %>%
    mutate(time.point = "Follow up 01" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_01 <- rbind (summary_table_sample_01,summary_table_01)

```


```{r built summary table 02}

summary_table_02 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_02,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_02,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_02)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 02" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_02 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_02,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_02,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_02)))  %>%
    mutate(time.point = "Follow up 02" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_02 <- rbind (summary_table_sample_02,summary_table_02)

```

```{r built summary table 03}

summary_table_03 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_03,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_03,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_03)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 03" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_03 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_03,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_03,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_03)))  %>%
    mutate(time.point = "Follow up 03" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_03 <- rbind (summary_table_sample_03,summary_table_03)

```

```{r built summary table 04}

summary_table_04 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_04,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_04,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_04)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 04" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_04 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_04,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_04,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_04)))  %>%
    mutate(time.point = "Follow up 04" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_04 <- rbind (summary_table_sample_04,summary_table_04)

```

```{r built summary table 05}

summary_table_05 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_05,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_05,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_05)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 05" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_05 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_05,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_05,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_05)))  %>%
    mutate(time.point = "Follow up 05" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_05 <- rbind (summary_table_sample_05,summary_table_05)

```

```{r built summary table 06}

summary_table_06 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_06,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_06,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_06)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 06" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_06 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_06,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_06,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_06)))  %>%
    mutate(time.point = "Follow up 06" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_06 <- rbind (summary_table_sample_06,summary_table_06)

```

```{r built summary table 07}

summary_table_07 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_07,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_07,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_07)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 07" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_07 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_07,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_07,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_07)))  %>%
    mutate(time.point = "Follow up 07" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_07 <- rbind (summary_table_sample_07,summary_table_07)

```

```{r built summary table 08}

summary_table_08 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_08,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_08,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_08)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 08" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_08 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_08,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_08,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_08)))  %>%
    mutate(time.point = "Follow up 08" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_08 <- rbind (summary_table_sample_08,summary_table_08)

```

```{r built summary table 09}

summary_table_09 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_09,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_09,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_09)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 09" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_09 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_09,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_09,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_09)))  %>%
    mutate(time.point = "Follow up 09" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_09 <- rbind (summary_table_sample_09,summary_table_09)

```

```{r built summary table 10}

summary_table_10 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_10,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_10,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_10)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 10" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_10 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_10,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_10,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_10)))  %>%
    mutate(time.point = "Follow up 10" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_10 <- rbind (summary_table_sample_10,summary_table_10)

```

```{r built summary table 11}

summary_table_11 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_11,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_11,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_11)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 11" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_11 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_11,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_11,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_11)))  %>%
    mutate(time.point = "Follow up 11" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_11 <- rbind (summary_table_sample_11,summary_table_11)

```

```{r built summary table 12}

summary_table_12 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_12,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_12,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_12)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 12" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_12 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_12,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_12,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_12)))  %>%
    mutate(time.point = "Follow up 12" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_12 <- rbind (summary_table_sample_12,summary_table_12)

```

```{r built summary table 13}

summary_table_13 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_13,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_13,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_13)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 13" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_13 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_13,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_13,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_13)))  %>%
    mutate(time.point = "Follow up 13" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_13 <- rbind (summary_table_sample_13,summary_table_13)

```

```{r built summary table 14}

summary_table_14 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_14,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_14,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_14)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 14" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_14 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_14,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_14,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_14)))  %>%
    mutate(time.point = "Follow up 14" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_14 <- rbind (summary_table_sample_14,summary_table_14)

```

```{r built summary table 15}

summary_table_15 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_15,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_15,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_15)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 15" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_15 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_15,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_15,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_15)))  %>%
    mutate(time.point = "Follow up 15" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_15 <- rbind (summary_table_sample_15,summary_table_15)

```

```{r built summary table 16}

summary_table_16 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_16,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_16,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_16)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 16" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_16 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_16,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_16,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_16)))  %>%
    mutate(time.point = "Follow up 16" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_16 <- rbind (summary_table_sample_16,summary_table_16)

```

```{r built summary table 17}

summary_table_17 <- 
  
  phq.totals.clean %>%
  summarise(mean = mean(phq.total_Wave_17,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_17,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_17)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 17" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_17 <- 
  
  phq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(phq.total_Wave_17,na.rm = TRUE),
            standard.deviation = sd(phq.total_Wave_17,na.rm = TRUE),
            valid.data.points =  sum(!is.na(phq.total_Wave_17)))  %>%
    mutate(time.point = "Follow up 17" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_17 <- rbind (summary_table_sample_17,summary_table_17)

```

## join all tables


```{r join all waves}

full_table <- rbind(table_time_0,table_time_01,table_time_02,table_time_03,table_time_04,table_time_05,
                    table_time_06,table_time_07,table_time_08,table_time_09,table_time_10,
                    table_time_11,table_time_12,table_time_13,table_time_14,table_time_15,
                    table_time_16,table_time_17)

print(full_table)
```
## merge with dates

```{r join all waves to date merge}

full_table <- full_join(full_table,date.merge)

print(full_table)
```

## prettify and save with KABLE (colour combined rows)

```{r create an index list to specify coloured rows}
colour.me <- which(full_table$sample == "Combined")
```

```{r make kable table}

pretty_table <- full_table %>%
  kable(booktabs = T,
        align='llccc',
        digits = 2,
        col.names = str_to_title(gsub("[.]", " ", names(full_table)))) %>%
  kable_styling() %>%
  row_spec(colour.me,bold=T,background = "lightgrey") %>%
  row_spec(0,bold=T,background = "grey",font_size = 16)

pretty_table 
```
```{r save kable}

save_file <- file.path(dirname(dirname(getwd())),"output/phq.all_waves_summary_table.html")

save_kable(pretty_table,
           save_file)

```


# examine data (total scores) for normality

## descriptives


```{r}

### Check distributions of raw data, square root transformed data and log transformed data (bring above 1 first...) using histograms

phq.totals.clean %>%
  describe(.)

```

##  histograms

```{r differentials_variable_transformation_histograms,fig.height=12,fig.width=8}

layout(matrix(c(1:18), nrow=6, byrow=T))

hist(phq.totals.clean$phq.total_Wave_0, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_01, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_02, breaks="FD")

hist(phq.totals.clean$phq.total_Wave_03, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_04, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_05, breaks="FD")

hist(phq.totals.clean$phq.total_Wave_06, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_07, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_08, breaks="FD")

hist(phq.totals.clean$phq.total_Wave_09, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_10, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_11, breaks="FD")

hist(phq.totals.clean$phq.total_Wave_12, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_13, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_14, breaks="FD")

hist(phq.totals.clean$phq.total_Wave_15, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_16, breaks="FD")
hist(phq.totals.clean$phq.total_Wave_17, breaks="FD")

layout(1)


```




