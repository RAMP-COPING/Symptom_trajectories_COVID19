---
title: "Clean MASQ data for all waves. Save a seperate RAMP baseline data for CFA in addition to all wave for GMM"
author: "K L Purves"
version_label: 
date: '`r format(Sys.time(), "%d %B, %Y")`'

output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: false
    number_sections: false
    highlight: monochrome
    theme: cerulean
code_folding: show


html_notebook:
  theme: cerulean
toc: yes
---

# Setup

```{r}
# clear global environment
remove(list = ls())
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      comment=NA,
                      prompt=FALSE,
                      cache=FALSE)
```

Note: load tidyverse last!
```{r Load packages}
if(!require(summarytools)){
  install.packages("summarytools")
  library(summarytools)
}
if(!require(ggplot2)){
  install.packages("ggplot2")
  library(ggplot2)
}

if(!require(psych)){
  install.packages("psych")
  library(psych)
}

if(!require(tidyverse)){
  install.packages("tidyverse")
  library(tidyverse)
}

if(!require(kableExtra)){
  install.packages("kableExtra")
  library(kableExtra)
}


if(!require(multiplex)){
  install.packages("multiplex")
  library(multiplex)
}

if(!require(modeest)){
  install.packages("modeest")
  library(modeest)
}
```

```{r}
# read in the files (save to environment)
source("../../../Credentials/paths_anhedonia_analysis.R")
```

## read in all functions from function library

```{r call in function library functions}
# source all functions in the function library folder
files.sources = paste0("../functions/",list.files("../functions"))
sapply(files.sources, source)

```

# COPING data preprocessing {.tabset}

Important note: COPING did not include the masq at baseline. 

## read in and rename

### COPING follow up A

```{r Read in COPING follow up A data}
# Read in the data
COPING_followupA <- readRDS(file = "/Users/katherineyoung/Documents/RAMP-COPING-longitudinal_common/data_clean/masq/masq_coping_followupa.rds")

  # Check column names
COPING_followupA %>%
  colnames()

# Check dimensions
COPING_followupA %>%
  dim()

# Look at top rows of the data frame
COPING_followupA %>% 
  head()
```

```{r}


COPING_followupA.id <- COPING_followupA %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs
  add_column(sample = "COPING_followupA", .after = "externalDataReference") %>% #create new column 
  select(
         ID = externalDataReference, # ID
         sample,
         startDate,
         endDate,
         masq.pandemic_felt_successful = masq.felt_successful,
         masq.pandemic_felt_really_happy = masq.felt_really_happy,
         masq.pandemic_felt_optimistic = masq.felt_optimistic,
         masq.pandemic_lot_felt_fun = masq.lot_felt_fun,
         masq.pandemic_felt_like_i_accomplished_a_lot = masq.felt_like_i_accomplished_a_lot,
         masq.pandemic_forward_lot_felt = masq.forward_lot_felt,
         masq.pandemic_felt_really_talkative = masq.felt_really_talkative,
         masq.pandemic_felt_really_up_or_lively = masq.felt_really_up_or_lively,
         masq.pandemic_lot_felt_energy = masq.lot_felt_energy,
         masq.pandemic_felt_really_good_about_myself = masq.felt_really_good_about_myself,
         masq.feelings_felt_before_pandemic = masq.pandemic_feelings_felt,
         masq.pre_pandemic_felt_successful = masq.felt_successful.1,
         masq.pre_pandemic_felt_really_happy = masq.felt_really_happy.1,
         masq.pre_pandemic_felt_optimistic = masq.felt_optimistic.1,
         masq.pre_pandemic_lot_felt_fun = masq.lot_felt_fun.1,
         masq.pre_pandemic_felt_like_i_accomplished_a_lot = masq.felt_like_i_accomplished_a_lot.1,
         masq.pre_pandemic_forward_lot_felt = masq.forward_lot_felt.1,
         masq.pre_pandemic_felt_really_talkative = masq.felt_really_talkative.1,
         masq.pre_pandemic_felt_really_up_or_lively = masq.felt_really_up_or_lively.1,
         masq.pre_pandemic_lot_felt_energy = masq.lot_felt_energy.1,
         masq.pre_pandemic_felt_really_good_about_myself = masq.felt_really_good_about_myself.1) 

# Inspect colnames
colnames(COPING_followupA.id)

```
  
### COPING follow up B

```{r Read in COPING follow up B data}
# Read in the data
COPING_followupB <- readRDS(file = paste0(ilovedata, "/data_raw/2021-05-20/coping_followupb/masq_coping_followupb.rds"))

# Check column names
COPING_followupB %>%
  colnames()

# Check dimensions
COPING_followupB %>%
  dim()

# Look at top rows of the data frame
COPING_followupB %>% 
  head()
```

Select & rename relevant columns (will be a function at some point)
```{r}


COPING_followupB.id <- COPING_followupB %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs
  add_column(sample = "COPING_followupB", .after = "externalDataReference") %>% #create new column 
  select(
         ID = externalDataReference, # ID
         sample,
         startDate,
         endDate,
         masq.pandemic_felt_successful = masq.felt_successful,
         masq.pandemic_felt_really_happy = masq.felt_really_happy,
         masq.pandemic_felt_optimistic = masq.felt_optimistic,
         masq.pandemic_lot_felt_fun = masq.fun_lot_felt,
         masq.pandemic_felt_like_i_accomplished_a_lot = masq.felt_like_i_accomplished_a_lot,
         masq.pandemic_forward_lot_felt = masq.forward_lot_felt,
         masq.pandemic_felt_really_talkative = masq.felt_really_talkative,
         masq.pandemic_felt_really_up_or_lively = masq.felt_really_up_or_lively,
         masq.pandemic_lot_felt_energy = masq.energy_lot_felt,
         masq.pandemic_felt_really_good_about_myself = masq.felt_really_good_about_myself) 



# Inspect colnames
colnames(COPING_followupB.id)

```

### COPING followup A ongoing
```{r}
# Read in the data
COPING_followupA_ong <- readRDS(file = paste0(ilovedata, "/data_raw/2021-05-20/coping_followupa_ongoing/masq_coping_followupa_ongoing.rds"))

# Check column names
COPING_followupA_ong %>%
  colnames()

# Check dimensions
COPING_followupA_ong %>%
  dim()

# Look at top rows of the data frame
COPING_followupA_ong %>% 
  head()
```

Select & rename relevant columns (will be a function at some point)
```{r}

COPING_followupA_ong.id <- COPING_followupA_ong %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs
  add_column(sample = "COPING_followupA_ong", .after = "externalDataReference") %>% #create new column 
  select(
         ID = externalDataReference, # ID
         sample,
         startDate,
         endDate,
         masq.pandemic_felt_successful = masq.felt_successful,
         masq.pandemic_felt_really_happy = masq.felt_really_happy,
         masq.pandemic_felt_optimistic = masq.felt_optimistic,
         masq.pandemic_lot_felt_fun = masq.lot_felt_fun,
         masq.pandemic_felt_like_i_accomplished_a_lot = masq.felt_like_i_accomplished_a_lot,
         masq.pandemic_forward_lot_felt = masq.forward_lot_felt,
         masq.pandemic_felt_really_talkative = masq.felt_really_talkative,
         masq.pandemic_felt_really_up_or_lively = masq.felt_really_up_or_lively,
         masq.pandemic_lot_felt_energy = masq.lot_felt_energy,
         masq.pandemic_felt_really_good_about_myself = masq.felt_really_good_about_myself) 



# Inspect colnames
colnames(COPING_followupA_ong.id)

```

## Creating follow up time points

### merge datasets
doing this before identifying wave dates as there are some people who have duplicated data per wave between datasets (e.g. if there were individual links sent to someone who needed to restart I assume). So need to get latest data for a wave from every possible point.


```{r merge coping datasets}

follow_up_list <- list(COPING_followupA.id,
                       COPING_followupA_ong.id,
                       COPING_followupB.id)

COPING_followup.id <- plyr::join_all(follow_up_list,
                                     type="full")
  
```


```{r}

# April/May (wave 1) - FOLLOW UP A
ramp_start1 <- as.POSIXct("2020-04-21")
ramp_end1 <-  as.POSIXct("2020-05-04")  

# May (wave 2) - FOLLOW UP B
ramp_start2 <- as.POSIXct("2020-05-05")
ramp_end2<-  as.POSIXct("2020-05-18")  


# MAY 2 (wave 3) - FOLLOW UP A
start1 <- as.POSIXct("2020-05-19")
end1 <-  as.POSIXct("2020-06-01")  

# JUNE 1 (wave 4) - FOLLOW UP B
start2 <- as.POSIXct("2020-06-02")
end2<-  as.POSIXct("2020-06-15")  

# JUNE 2 (wave 5) - FOLLOW UP A
start3 <- as.POSIXct("2020-06-16")
end3 <-  as.POSIXct("2020-06-29") 

# JULY 1 (wave 6) - FOLLOW UP B
start4 <- as.POSIXct("2020-06-30")
end4<-  as.POSIXct("2020-07-13") 

# JULY 2 (wave 7) - FOLLOW UP A
start5 <- as.POSIXct("2020-07-14")
end5 <-  as.POSIXct("2020-07-27") ## Ongoing starts from here?

# JULY - AUG (wave 8) - FOLLOW UP B
start6 <- as.POSIXct("2020-07-28") 
end6 <-  as.POSIXct("2020-08-24") 

# AUG - SEPT (wave 9) - FOLLOW UP A
start7 <- as.POSIXct("2020-08-25")
end7 <-  as.POSIXct("2020-09-21") 

# SEPT - OCT (wave 10) - FOLLOW UP B
start8 <- as.POSIXct("2020-09-22")
end8 <-  as.POSIXct("2020-10-19") 

# OCT - NOV (wave 11) - FOLLOW UP A
start9 <- as.POSIXct("2020-10-20")
end9 <-  as.POSIXct("2020-11-16") 

# NOV - DEC (wave 12) - FOLLOW UP B
start10 <- as.POSIXct("2020-11-17")
end10 <-  as.POSIXct("2020-12-14") 

# DEC - JAN (wave 13) - FOLLOW UP A
start11 <- as.POSIXct("2020-12-15")
end11 <-  as.POSIXct("2021-01-11") 

# JAN - FEB (wave 14) - FOLLOW UP B
start12 <- as.POSIXct("2021-01-12")
end12 <-  as.POSIXct("2021-02-08") 

# FEB - MARCH (wave 15) - FOLLOW UP A
start13 <- as.POSIXct("2021-02-09")
end13 <-  as.POSIXct("2021-03-08") 

# MARCH - APR (wave 16) - FOLLOW UP B
start14 <- as.POSIXct("2021-03-09")
end14 <-  as.POSIXct("2021-04-05") 

# APR - MAY (wave 17) - FOLLOW UP A
start15 <- as.POSIXct("2021-04-06")
end15 <-  as.POSIXct("2021-05-03")

# MAY - JUNE (wave 18) - FOLLOW UP B
start16 <- as.POSIXct("2021-05-04")
end16 <-  as.POSIXct("2021-05-31")

# JUNE - JULY (wave 19) - FOLLOW UP A
start17 <- as.POSIXct("2021-06-01")
end17 <-  as.POSIXct("2021-06-28")

# JUNE - JULY (wave 20) - FOLLOW UP B
start18 <- as.POSIXct("2021-06-29")
end18 <-  as.POSIXct("2021-07-26")

# JUNE - JULY (wave 21) - FOLLOW UP A
start19 <- as.POSIXct("2021-07-27")
end19 <-  as.POSIXct("2021-08-17")


COPING_followup.id <- COPING_followup.id %>%
  mutate(wave = case_when(startDate >= start1 & startDate < end1 ~ ".Wave_03",
                           startDate >= start2 & startDate <= end2 ~ ".Wave_04",
                            startDate >= start3 & startDate <= end3 ~ ".Wave_05", 
                           startDate >= start4 & startDate < start5 ~ ".Wave_06",
                            startDate >= start5 & startDate < start6 ~ ".Wave_07",
                           startDate >= start6 & startDate <= end6 ~ ".Wave_08",
                            startDate >= start7 & startDate <= end7 ~ ".Wave_09",
                           startDate >= start8 & startDate <= end8 ~ ".Wave_10",
                            startDate >= start9 & startDate <= end9 ~ ".Wave_11",
                           startDate >= start10 & startDate <= end10 ~ ".Wave_12",
                            startDate >= start11 & startDate <= end11 ~ ".Wave_13",
                           startDate >= start12 & startDate <= end12 ~ ".Wave_14",
                            startDate >= start13 & startDate <= end13 ~ ".Wave_15",
                              startDate >= start14 & startDate <= end14 ~ ".Wave_16",
                            startDate >= start15 & startDate <= end15 ~ ".Wave_17",
                            startDate >= start16 & startDate <= end16 ~ ".Wave_18",
                           startDate >= start17 & startDate <= end17 ~ ".Wave_19",
                           startDate >= start18 & startDate <= end18 ~ ".Wave_20",
                            startDate >= start19 & startDate <= end19 ~ ".Wave_21"))


```

### check for number of entries per wave

```{r check for number of entries per wave COPING}

COPING_followup.id %>%
  group_by(wave) %>%
  count()
```

### Identifying duplicate IDs in a single wave
```{r Identifying duplicate IDs in a single wave COPING}


##Identify dup IDs in a single wave (all) 
COPING_followup.id %>%
   group_by(wave, ID) %>%
   dplyr::summarize(N = n()) %>%
   filter(N > 1)



```

**removing duplicates, retaining the latest possible data where there are repeats**

### Retain most recent completion where duplicate completions
We want there to be unique IDs within each of the waves
```{r}
##FOLLOW UP 
##confirm number of rows in current dataset
nrow(COPING_followup.id)

#Filter out duplicated IDs AND wave, keeping the LAST entry
COPING_followup.id <- COPING_followup.id %>% 
    group_by(ID) %>% 
    filter(!duplicated(wave,
                      fromLast = TRUE)
           )

##confirm number of rows in dataset after filtering (should be 22485)
nrow(COPING_followup.id)


```


## Change COPING follow up A and B from long to wide format (we want this for the numeric ones so we can sum them)
```{r COPING follow up A and B long to wide format}

COPING_followup.id.long <- COPING_followup.id %>%
  group_by(wave) %>%
  tidyr::pivot_wider(id_cols = ID, 
    names_from = wave, 
                     values_from = c(masq.pandemic_felt_successful,
                                     masq.pandemic_felt_really_happy,
                                     masq.pandemic_felt_optimistic,
                                     masq.pandemic_lot_felt_fun,
                                     masq.pandemic_felt_like_i_accomplished_a_lot,
                                     masq.pandemic_forward_lot_felt,
                                     masq.pandemic_felt_really_talkative,
                                     masq.pandemic_felt_really_up_or_lively,
                                     masq.pandemic_lot_felt_energy,
                                     masq.pandemic_felt_really_good_about_myself)) 

colnames(COPING_followup.id.long)


```
  
# Clean COPING data {.tabset}

## Set minimum and maximum values 
```{r Set minimum and maximum values}
masq.min.scale <- 1
masq.max.scale <- 5
```



## COPING follow up

```{r Add "_unc" to the end of all variables COPING follow up}
COPING_followup.id.long <- COPING_followup.id.long %>% 
  rename_with( ~ paste(.x, "unc", sep = "_"), starts_with("masq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(COPING_followup.id.long)
```

```{r COPING follow up inspect the variables}
COPING_followup.id.long %>% 
  freq(masq.pandemic_felt_successful_.Wave_03_unc)
```

### Make list of variables for cleaning
```{r Make list of variables for cleaning COPING }
masq.items.followup_unc <- COPING_followup.id.long %>% 
  select(contains("_unc")) %>% 
  colnames()

masq.items.followup_unc
```

### Recode implausuble values
```{r Recode implausible values COPING}
# Recode and clean
COPING_followup.id.long <- COPING_followup.id.long %>%
   mutate(
     across(all_of(masq.items.followup_unc),
            .fns = list(clean = ~case_when( # add "_clean" onto the variables you are cleaning
              . == -77 | . == -88 | . == -99 ~ ., # leave as is 
              . < masq.min.scale | . > masq.max.scale ~ -66, # recode as implausible value
              TRUE ~ . # leave as is
            )
            )
     )
   ) %>%
   rename_at(
     vars(contains( "_unc_clean")), # specify variables that now contain "_clean"
     list(~paste0(gsub("_unc_clean", "", .))) # remove the "_clean" now that they have been cleaned
   )

# Check
colnames(COPING_followup.id.long)
```
### Make new list of cleaned variables
```{r}
masq.items.followup_clean <- COPING_followup.id.long %>% 
  select(!contains("_unc")) %>% # select the cleaned variables (i.e. they don't contain the phrase "_unc")
  select(!"ID") %>% # deselect the ID variable
  colnames()

masq.items.followup_clean
```
### Check number of implausible values
```{r}
# Check for implausible values
masq_followup_imp_n <- COPING_followup.id.long %>% 
  select(all_of(masq.items.followup_clean)) %>% 
  filter(. == -66) %>% 
  nrow()

# If statement
if (masq_followup_imp_n == 0) {
  print(paste0("The number of implausible values in the coping follow up A masq variables is ", masq_followup_imp_n, ". Can leave these variables as they are."))
} else {
  print(paste0("The number of implausible values in the coping follow up A masq variables is ", masq_followup_imp_n, ". Please investigate."))
}
```

### Select only the clean variables
Note: keep ID in this data set
```{r}
COPING_followup.id.long.clean <- COPING_followup.id.long %>% 
  select(!contains("_unc")) 

colnames(COPING_followup.id.long.clean)
```

# Reorder COPING survey by waves
Note: full join (i.e. to get all participants in all three data sets)
```{r reorderCOPING follow up surveys}
# reorder so that it is easy to see each wave in place.
masq.COPING.reordered <- COPING_followup.id.long.clean %>% 
  select(order(names(COPING_followup.id.long.clean)))

dim(masq.COPING.reordered)
colnames(masq.COPING.reordered)
```
  
# RAMP data preprocessing {.tabset}
## RAMP baseline 
```{r}
# read in baseline data
RAMP_baseline = readRDS(paste0(ilovedata,"/data_raw/2021-02-18/ramp/masq_ramp.rds"))

# Check column names
RAMP_baseline %>%
  colnames()

# Check dimensions
RAMP_baseline %>%
  dim()

# Look at top rows of the data frame
RAMP_baseline %>% 
  head()
```

```{r}
#Recode column names to harmonised variable names


RAMP_baseline.id <- RAMP_baseline %>% #new dataset with ID
  drop_na("Login ID") %>% # Drop NAs
  select(
         ID = "Login ID", # ID
         startDate,
         endDate,
         masq.pandemic_felt_successful = masq.felt_successful,
         masq.pandemic_felt_really_happy = masq.felt_really_happy,
         masq.pandemic_felt_optimistic = masq.felt_optimistic,
         masq.pandemic_lot_felt_fun = masq.fun_lot_felt,
         masq.pandemic_felt_like_i_accomplished_a_lot = masq.felt_like_i_accomplished_a_lot,
         masq.pandemic_forward_lot_felt = masq.forward_lot_felt,
         masq.pandemic_felt_really_talkative = masq.felt_really_talkative,
         masq.pandemic_felt_really_up_or_lively = masq.felt_really_up_or_lively,
         masq.pandemic_lot_felt_energy = masq.energy_lot_felt,
         masq.pandemic_felt_really_good_about_myself = masq.felt_really_good_about_myself)
         

# Inspect colnames
colnames(RAMP_baseline.id)


```


```{r}
#seen but not answered (-99 & -77 ) as NA
RAMP_baseline.id <- RAMP_baseline.id %>%
  mutate_at(vars(starts_with("masq")),
            funs(case_when(

             . == -77 ~ NA_real_,
             TRUE ~  .)))
  

```

## RAMP follow up A

```{r Read in RAMP follow up A data}
# Read in the data
RAMP_followupA <- readRDS(file = paste0(ilovedata, "/data_raw/2021-04-09/ramp_followupa/masq_ramp_followupa.rds"))

# Check column names
RAMP_followupA %>%
  colnames()

# Check dimensions
RAMP_followupA %>%
  dim()

# Look at top rows of the data frame
RAMP_followupA %>% 
  head()
```


```{r }

RAMP_followupA.id <- RAMP_followupA %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs
  add_column(sample = "RAMP_followupA", .after = "externalDataReference") %>% #create new column 
  select(
         ID = externalDataReference, # ID
         sample,
         startDate,
         endDate,
         masq.pandemic_felt_successful = masq.felt_successful,
         masq.pandemic_felt_really_happy = masq.felt_really_happy,
         masq.pandemic_felt_optimistic = masq.felt_optimistic,
         masq.pandemic_lot_felt_fun = masq.lot_felt_fun,
         masq.pandemic_felt_like_i_accomplished_a_lot = masq.felt_like_i_accomplished_a_lot,
         masq.pandemic_forward_lot_felt = masq.forward_lot_felt,
         masq.pandemic_felt_really_talkative = masq.felt_really_talkative,
         masq.pandemic_felt_really_up_or_lively = masq.felt_really_up_or_lively,
         masq.pandemic_lot_felt_energy = masq.lot_felt_energy,
         masq.pandemic_felt_really_good_about_myself = masq.felt_really_good_about_myself)


# Inspect colnames
colnames(RAMP_followupA.id)

```
  

## RAMP follow up B

```{r Read in RAMP follow up B data}
# Read in the data
RAMP_followupB <- readRDS(file = paste0(ilovedata, "/data_raw/2021-04-09/ramp_followupb/masq_ramp_followupb.rds"))

# Check column names
RAMP_followupB %>%
  colnames()

# Check dimensions
RAMP_followupB %>%
  dim()

# Look at top rows of the data frame
RAMP_followupB %>% 
  head()
```

Select & rename relevant columns (will be a function at some point)
```{r}


RAMP_followupB.id <- RAMP_followupB %>% #new dataset with ID
  drop_na(externalDataReference) %>% # Drop NAs
  add_column(sample = "RAMP_followupB", .after = "externalDataReference") %>% #create new column 
  select(
         ID = externalDataReference, # ID
         sample,
         startDate,
         endDate,
         masq.pandemic_felt_successful = masq.felt_successful,
         masq.pandemic_felt_really_happy = masq.felt_really_happy,
         masq.pandemic_felt_optimistic = masq.felt_optimistic,
         masq.pandemic_lot_felt_fun = masq.fun_lot_felt,
         masq.pandemic_felt_like_i_accomplished_a_lot = masq.felt_like_i_accomplished_a_lot,
         masq.pandemic_forward_lot_felt = masq.forward_lot_felt,
         masq.pandemic_felt_really_talkative = masq.felt_really_talkative,
         masq.pandemic_felt_really_up_or_lively = masq.felt_really_up_or_lively,
         masq.pandemic_lot_felt_energy = masq.energy_lot_felt,
         masq.pandemic_felt_really_good_about_myself = masq.felt_really_good_about_myself) 



# Inspect colnames
colnames(RAMP_followupB.id)

```

## Creating follow up time points RAMP 

### Create waves within the FOLLOW UP DATA 
Note: dates from google sheet from Kirstin
Checked with Gerome - set start date as the date the survey was released, and keep window open until the DAY BEFORE the next survey was released

these are in order, incorporating everyone into wave according to date of completion as per the COPING above. 

```{r}

# April/May (wave 1) - FOLLOW UP A
ramp_start1 <- as.POSIXct("2020-04-21")
ramp_end1 <-  as.POSIXct("2020-05-04")  

# May (wave 2) - FOLLOW UP B
ramp_start2 <- as.POSIXct("2020-05-05")
ramp_end2<-  as.POSIXct("2020-05-18")  

# May/June (wave 3) - FOLLOW UP A
ramp_start3 <- as.POSIXct("2020-05-19")
ramp_end3 <-  as.POSIXct("2020-06-01") 

# June 1 (wave 4) - FOLLOW UP B
ramp_start4 <- as.POSIXct("2020-06-02")
ramp_end4<-  as.POSIXct("2020-06-15") 

# June 2 (wave 5) - FOLLOW UP A
ramp_start5 <- as.POSIXct("2020-06-16")
ramp_end5 <-  as.POSIXct("2020-06-29") 

# June/July (wave 6) - FOLLOW UP B
ramp_start6 <- as.POSIXct("2020-06-30") 
ramp_end6 <-  as.POSIXct("2020-07-13") 

# July 1 (wave 7) - FOLLOW UP A
ramp_start7 <- as.POSIXct("2020-07-14")
ramp_end7 <-  as.POSIXct("2020-07-27") 

# July/August (wave 8) - FOLLOW UP B
ramp_start8 <- as.POSIXct("2020-07-28")
ramp_end8 <-  as.POSIXct("2020-08-24") 

# August/September (wave 9) - FOLLOW UP A
ramp_start9 <- as.POSIXct("2020-08-25")
ramp_end9 <-  as.POSIXct("2020-09-21") 

# September/October (wave 10) - FOLLOW UP B
ramp_start10 <- as.POSIXct("2020-09-22")
ramp_end10 <-  as.POSIXct("2020-10-19") 

# October/November (wave 11) - FOLLOW UP A
ramp_start11 <- as.POSIXct("2020-10-20")
ramp_end11 <-  as.POSIXct("2020-11-16") 

# November/December (wave 12) - FOLLOW UP B
ramp_start12 <- as.POSIXct("2020-11-17")
ramp_end12 <-  as.POSIXct("2020-12-14") 

# December/January (wave 13) - FOLLOW UP A
ramp_start13 <- as.POSIXct("2020-12-15")
ramp_end13 <-  as.POSIXct("2021-01-18") 

# January/February (wave 14) - FOLLOW UP B
ramp_start14 <- as.POSIXct("2021-01-19")
ramp_end14 <-  as.POSIXct("2021-02-15") 

# February/March (wave 15) - FOLLOW UP A
ramp_start15 <- as.POSIXct("2021-02-16")
ramp_end15 <-  as.POSIXct("2021-03-15")

# March/April (wave 16) - FOLLOW UP B
ramp_start16 <- as.POSIXct("2021-03-16")
ramp_end16 <-  as.POSIXct("2021-04-19")

# April/May (wave 17) - FOLLOW UP A
ramp_start17 <- as.POSIXct("2021-04-20")
ramp_end17 <-  as.POSIXct("2021-05-10")

# May (wave 18) - FOLLOW UP B
ramp_start18 <- as.POSIXct("2021-05-11")
ramp_end18 <-  as.POSIXct("2021-05-31")

# June (wave 19) - FOLLOW UP A
ramp_start19 <- as.POSIXct("2021-06-01")
ramp_end19 <-  as.POSIXct("2021-06-28")


#JM**do we have data beyond this?
## KLP corrected the below to ensure 6 people who were NA (1 in wave A, 5 in wave B) are correctly allocated to the correct wave. easy check for dates for these wave NAs is to run the following after you run these chunks the first time: RAMP_followupA.id$endDate[is.na(RAMP_followupA.id$waveA)]

RAMP_followupA.id <- RAMP_followupA.id %>%
  mutate(waveA = case_when(startDate >= ramp_start1 & startDate < ramp_start2 ~ ".Wave_01",
                           startDate >= ramp_start2 & startDate <= ramp_end2 ~ ".Wave_02",
                            startDate >= ramp_start3 & startDate <= ramp_end3 ~ ".Wave_03",
                           startDate >= ramp_start4 & startDate <= ramp_end4 ~ ".Wave_04",
                            startDate >= ramp_start5 & startDate <= ramp_end5 ~ ".Wave_05",
                           startDate >= ramp_start6 & startDate <= ramp_end6 ~ ".Wave_06",
                            startDate >= ramp_start7 & startDate <= ramp_end7 ~ ".Wave_07",
                           startDate >= ramp_start8 & startDate <= ramp_end8 ~ ".Wave_08",
                            startDate >= ramp_start9 & startDate <= ramp_end9 ~ ".Wave_09",
                           startDate >= ramp_start10 & startDate <= ramp_end10 ~ ".Wave_10",
                            startDate >= ramp_start11 & startDate <= ramp_end11 ~ ".Wave_11",
                           startDate >= ramp_start12 & startDate <= ramp_end12 ~ ".Wave_12",
                            startDate >= ramp_start13 & startDate <= ramp_end13 ~ ".Wave_13",
                              startDate >= ramp_start14 & startDate <= ramp_end14 ~ ".Wave_14",
                            startDate >= ramp_start15 & startDate <= ramp_end15 ~ ".Wave_15",
                            startDate >= ramp_start16 & startDate <= ramp_end16 ~ ".Wave_16",
                           startDate >= ramp_start17 & startDate <= ramp_end17 ~ ".Wave_17",
                           startDate >= ramp_start18 & startDate <= ramp_end18 ~ ".Wave_18",
                            startDate >= ramp_start19 & startDate <= ramp_end19 ~ ".Wave_19")
         )

RAMP_followupB.id <- RAMP_followupB.id %>%
  mutate(waveB = case_when(startDate >= ramp_start1 & startDate < ramp_end1 ~ ".Wave_1A_in_B", # should be impossible to exist
                           startDate >= ramp_start2 & startDate < ramp_start3 ~ ".Wave_02",
                            startDate >= ramp_start3 & startDate <= ramp_end3 ~ ".Wave_03",
                           startDate >= ramp_start4 & startDate <= ramp_end4 ~ ".Wave_04",
                            startDate >= ramp_start5 & startDate <= ramp_end5 ~ ".Wave_05",
                           startDate >= ramp_start6 & startDate <= ramp_end6 ~ ".Wave_06",
                            startDate >= ramp_start7 & startDate <= ramp_end7 ~ ".Wave_07",
                           startDate >= ramp_start8 & startDate <= ramp_end8 ~ ".Wave_08",
                            startDate >= ramp_start9 & startDate <= ramp_end9 ~ ".Wave_09",
                           startDate >= ramp_start10 & startDate <= ramp_end10 ~ ".Wave_10",
                            startDate >= ramp_start11 & startDate <= ramp_end11 ~ ".Wave_11",
                           startDate >= ramp_start12 & startDate <= ramp_end12 ~ ".Wave_12",
                            startDate >= ramp_start13 & startDate <= ramp_end13 ~ ".Wave_13",
                              startDate >= ramp_start14 & startDate <= ramp_end14 ~ ".Wave_14",
                            startDate >= ramp_start15 & startDate <= ramp_end15 ~ ".Wave_15",
                            startDate >= ramp_start16 & startDate <= ramp_end16 ~ ".Wave_16",
                           startDate >= ramp_start17 & startDate <= ramp_end17 ~ ".Wave_17",
                           startDate >= ramp_start18 & startDate <= ramp_end18 ~ ".Wave_18",
                            startDate >= ramp_start19 & startDate <= ramp_end19 ~ ".Wave_19"))



```


### check for number of entries per wave

```{r check for number of entries per wave RAMP}

RAMP_followupA.id %>%
  group_by(waveA) %>%
  count()


RAMP_followupB.id %>%
  group_by(waveB) %>%
  count()
```

### Identifying duplicate IDs in a single wave
```{r Identifying duplicate IDs in a single wave RAMP}

##Identify dup IDs in a single wave (baseline)
RAMP_baseline.id %>%
   group_by(ID) %>%
   summarize(N = n()) %>%
   filter(N > 1)

##Identify dup IDs in a single wave (follow up A)
RAMP_followupA.id %>%
   group_by(waveA, ID) %>%
   summarize(N = n()) %>%
   filter(N > 1)

##Identify dup IDs in a single wave (B) 
RAMP_followupB.id %>%
   group_by(waveB, ID) %>%
   summarize(N = n()) %>%
   filter(N > 1)


```

**removing duplicates, retaining the latest possible data where there are repeats**

### Want the LATER data entry from people who answered twice within a single wave 
We want there to be unique IDs within each of the waves
```{r}

##baseline

##confirm number of rows in current dataset
nrow(RAMP_baseline.id)

#Filter out duplicated IDs AND wave, keeping the LAST entry
RAMP_baseline.id <- RAMP_baseline.id %>% 
    group_by(ID) %>% 
    filter(!duplicated(ID,
                       fromLast = TRUE)
          
           )  %>%
             ungroup()

##confirm number of rows in dataset after filtering 
nrow(RAMP_baseline.id)


##FOLLOW UP A
##confirm number of rows in current dataset
nrow(RAMP_followupA.id)

#Filter out duplicated IDs AND wave, keeping the LAST entry
RAMP_followupA.id <- RAMP_followupA.id %>% 
    group_by(ID) %>% 
    filter(!duplicated(waveA,
                      fromLast = TRUE)
           )

##confirm number of rows in dataset after filtering 
nrow(RAMP_followupA.id)

##FOLLOW UP B
##confirm number of rows in current dataset 
nrow(RAMP_followupB.id)

#Filter out duplicated IDs AND wave, keeping the LAST entry
RAMP_followupB.id <- RAMP_followupB.id %>% 
    group_by(ID) %>% 
    filter(!duplicated(waveB,
                      fromLast = TRUE)
           )

##confirm number of rows in dataset after filtering 
nrow(RAMP_followupB.id)
```

## Change RAMP follow up A and B from long to wide format (we want this for the numeric ones so we can sum them)
```{r RAMP follow up A and B long to wide format}

RAMP_followupA.id.long <- RAMP_followupA.id %>%
  group_by(waveA) %>%
  tidyr::pivot_wider(id_cols = ID, 
    names_from = waveA, 
                     values_from = c(masq.pandemic_felt_successful,
                                     masq.pandemic_felt_really_happy,
                                     masq.pandemic_felt_optimistic,
                                     masq.pandemic_lot_felt_fun,
                                     masq.pandemic_felt_like_i_accomplished_a_lot,
                                     masq.pandemic_forward_lot_felt,
                                     masq.pandemic_felt_really_talkative,
                                     masq.pandemic_felt_really_up_or_lively,
                                     masq.pandemic_lot_felt_energy,
                                     masq.pandemic_felt_really_good_about_myself)) 


RAMP_followupB.id.long <- RAMP_followupB.id %>%
  group_by(waveB) %>%
  tidyr::pivot_wider(id_cols = ID, 
    names_from = waveB, 
                     values_from = c(masq.pandemic_felt_successful,
                                     masq.pandemic_felt_really_happy,
                                     masq.pandemic_felt_optimistic,
                                     masq.pandemic_lot_felt_fun,
                                     masq.pandemic_felt_like_i_accomplished_a_lot,
                                     masq.pandemic_forward_lot_felt,
                                     masq.pandemic_felt_really_talkative,
                                     masq.pandemic_felt_really_up_or_lively,
                                     masq.pandemic_lot_felt_energy,
                                     masq.pandemic_felt_really_good_about_myself)) 
  

# Check
colnames(RAMP_followupA.id.long)
colnames(RAMP_followupB.id.long)


```



# clean RAMP data {.tabset}

## RAMP Baseline
add wave 0 to the end of all variable names to indicate baseline
```{r Add "_.Wave_0" to the end of all variables RAMP baseline}
RAMP_baseline.id <- RAMP_baseline.id %>% 
  rename_with( ~ paste(.x, ".Wave_0", sep = "_"), starts_with("masq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(RAMP_baseline.id)
```

```{r Add "_unc" to the end of all variables RAMP baseline}
RAMP_baseline.id <- RAMP_baseline.id %>% 
  rename_with( ~ paste(.x, "unc", sep = "_"), starts_with("masq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(RAMP_baseline.id)
```

```{r RAMP Baseline inspect the variables}
RAMP_baseline.id %>% 
  freq(masq.pandemic_felt_successful_.Wave_0_unc)
```

### Make list of variables for cleaning
```{r Make list of variables for cleaning RAMP baseline}
masq.items.baseline_unc <- RAMP_baseline.id %>% 
  select(contains("_unc")) %>% 
  colnames()

masq.items.baseline_unc
```

### Recode implausuble values
```{r Recode implausible values RAMP baseline}
# Recode and clean
RAMP_baseline.id <- RAMP_baseline.id %>%
   mutate(
     across(all_of(masq.items.baseline_unc),
            .fns = list(clean = ~case_when( # add "_clean" onto the variables you are cleaning
              . == -77 | . == -88 | . == -99 ~ ., # leave as is 
              . < masq.min.scale | . > masq.max.scale ~ -66, # recode as implausible value
              TRUE ~ . # leave as is
            )
            )
     )
   ) %>%
   rename_at(
     vars(contains( "_unc_clean")), # specify variables that now contain "_clean"
     list(~paste0(gsub("_unc_clean", "", .))) # remove the "_clean" now that they have been cleaned
   )

# Check
colnames(RAMP_baseline.id)
```
### Make new list of cleaned variables
```{r}
masq.items.baseline_clean <- RAMP_baseline.id %>% 
  select(!contains("_unc")) %>% # select the cleaned variables (i.e. they don't contain the phrase "_unc")
  select(!"ID") %>% # deselect the ID variable
  colnames()

masq.items.baseline_clean
```
### Check number of implausible values
```{r}
# Check for implausible values
masq_baseline_imp_n <- RAMP_baseline.id %>% 
  select(all_of(masq.items.baseline_clean)) %>% 
  filter(. == -66) %>% 
  nrow()

# If statement
if (masq_baseline_imp_n == 0) {
  print(paste0("The number of implausible values in the RAMP Baseline MASQ variables is ", masq_baseline_imp_n, ". Can leave these variables as they are."))
} else {
  print(paste0("The number of implausible values in the RAMP Baseline MASQ variables is ", masq_baseline_imp_n, ". Please investigate."))
}
```

### Select only the clean variables
Note: keep ID in this data set
```{r}
RAMP_baseline.id.clean <- RAMP_baseline.id %>% 
  select(!contains("_unc")) 

colnames(RAMP_baseline.id.clean)
```


## RAMP follow up A

```{r Add "_unc" to the end of all variablesRAMP follow up A}
RAMP_followupA.id.long <- RAMP_followupA.id.long %>% 
  rename_with( ~ paste(.x, "unc", sep = "_"), starts_with("masq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(RAMP_followupA.id.long)
```

```{r RAMP follow up A inspect the variables}
RAMP_followupA.id.long %>% 
  freq(masq.pandemic_felt_successful_.Wave_01_unc)
```

### Make list of variables for cleaning
```{r Make list of variables for cleaningRAMP A}
masq.items.followupA_unc <- RAMP_followupA.id.long %>% 
  select(contains("_unc")) %>% 
  colnames()

masq.items.followupA_unc
```

### Recode implausuble values
```{r Recode implausible values RAMP A}
# Recode and clean
RAMP_followupA.id.long <- RAMP_followupA.id.long %>%
   mutate(
     across(all_of(masq.items.followupA_unc),
            .fns = list(clean = ~case_when( # add "_clean" onto the variables you are cleaning
              . == -77 | . == -88 | . == -99 ~ ., # leave as is 
              . < masq.min.scale | . > masq.max.scale ~ -66, # recode as implausible value
              TRUE ~ . # leave as is
            )
            )
     )
   ) %>%
   rename_at(
     vars(contains( "_unc_clean")), # specify variables that now contain "_clean"
     list(~paste0(gsub("_unc_clean", "", .))) # remove the "_clean" now that they have been cleaned
   )

# Check
colnames(RAMP_followupA.id.long)
```


### Make new list of cleaned variables
```{r}
masq.items.followupA_clean <- RAMP_followupA.id.long %>% 
  select(!contains("_unc")) %>% # select the cleaned variables (i.e. they don't contain the phrase "_unc")
  select(!"ID") %>% # deselect the ID variable
  colnames()

masq.items.followupA_clean
```
### Check number of implausible values
```{r}
# Check for implausible values
masq_followupA_imp_n <- RAMP_followupA.id.long %>% 
  select(all_of(masq.items.followupA_clean)) %>% 
  filter(. == -66) %>% 
  nrow()

# If statement
if (masq_followupA_imp_n == 0) {
  print(paste0("The number of implausible values in the RAMP follow up A MASQ variables is ", masq_followupA_imp_n, ". Can leave these variables as they are."))
} else {
  print(paste0("The number of implausible values in the RAMP follow up A MASQ variables is ", masq_followupA_imp_n, ". Please investigate."))
}
```

### Select only the clean variables
Note: keep ID in this data set
```{r}
RAMP_followupA.id.long.clean <- RAMP_followupA.id.long %>% 
  select(!contains("_unc")) 

colnames(RAMP_followupA.id.long.clean)
```

## RAMP follow up B

```{r Add "_unc" to the end of all variables RAMP follow up B}
RAMP_followupB.id.long <- RAMP_followupB.id.long %>% 
  rename_with( ~ paste(.x, "unc", sep = "_"), starts_with("masq")) # Add suffix "unc" for all uncleaned data columns

# check
colnames(RAMP_followupB.id.long)
```

```{r RAMP follow up B inspect the variables}
RAMP_followupB.id.long %>% 
  freq(masq.pandemic_felt_successful_.Wave_02_unc)
```

### Make list of variables for cleaning
```{r Make list of variables for cleaning RAMP B}
masq.items.followupB_unc <- RAMP_followupB.id.long %>% 
  select(contains("_unc")) %>% 
  colnames()

masq.items.followupB_unc
```

### Recode implausuble values
```{r Recode implausible values RAMP B}
# Recode and clean
RAMP_followupB.id.long <- RAMP_followupB.id.long %>%
   mutate(
     across(all_of(masq.items.followupB_unc),
            .fns = list(clean = ~case_when( # add "_clean" onto the variables you are cleaning
              . == -77 | . == -88 | . == -99 ~ ., # leave as is 
              . < masq.min.scale | . > masq.max.scale ~ -66, # recode as implausible value
              TRUE ~ . # leave as is
            )
            )
     )
   ) %>%
   rename_at(
     vars(contains( "_unc_clean")), # specify variables that now contain "_clean"
     list(~paste0(gsub("_unc_clean", "", .))) # remove the "_clean" now that they have been cleaned
   )

# Check
colnames(RAMP_followupB.id.long)
```
### Make new list of cleaned variables
```{r}
masq.items.followupB_clean <- RAMP_followupB.id.long %>% 
  select(!contains("_unc")) %>% # select the cleaned variables (i.e. they don't contain the phrase "_unc")
  select(!"ID") %>% # deselect the ID variable
  colnames()

masq.items.followupB_clean
```
### Check number of implausible values
```{r}
# Check for implausible values
masq_followupB_imp_n <- RAMP_followupB.id.long %>% 
  select(all_of(masq.items.followupB_clean)) %>% 
  filter(. == -66) %>% 
  nrow()

# If statement
if (masq_followupB_imp_n == 0) {
  print(paste0("The number of implausible values in the RAMP follow up B MASQ variables is ", masq_followupB_imp_n, ". Can leave these variables as they are."))
} else {
  print(paste0("The number of implausible values in the RAMP follow up B MASQ variables is ", masq_followupB_imp_n, ". Please investigate."))
}
```

### Select only the clean variables
Note: keep ID in this data set
```{r}
RAMP_followupB.id.long.clean <- RAMP_followupB.id.long %>% 
  select(!contains("_unc")) 

colnames(RAMP_followupB.id.long.clean)
```

# Save baseline RAMP cleaned data items only for CFA
```{r}

saveRDS(object = RAMP_baseline.id.clean, file = paste0("../../../data_clean/masq/masq.clean_baseline_items.RAMP_notrecoded",  ".rds"))

```

# Merge RAMP baseline, follow up A and B surveys
Note: full join (i.e. to get all participants in all three data sets)
```{r Merge RAMP follow up A and B surveys}
masq.merging <- list(RAMP_baseline.id.clean,
                       RAMP_followupA.id.long.clean,
                       RAMP_followupB.id.long.clean)


masq.RAMP <- plyr::join_all(masq.merging,
                    by = "ID",
                    type = "full")

  
# check
dim(RAMP_followupA.id.long.clean)
dim(RAMP_followupB.id.long.clean)
dim(RAMP_baseline.id.clean)
dim(masq.RAMP)

colnames(masq.RAMP)


# reorder so that it is easy to see each wave in place.
masq.RAMP.reordered <- masq.RAMP %>% 
  select(order(names(masq.RAMP))) 

colnames(masq.RAMP.reordered)

```

# merge COPING and RAMP all wave data

Merge by all common columns so that we dont duplicate waves. This is effectively an rbind action (adding a row for every case but keeping columns standard across)

```{r merge RAMP and COPING}

# add a sample columns
masq.RAMP.for.merging <- masq.RAMP.reordered %>%
  mutate(sample = "RAMP") %>%
  relocate(sample, .after = ID)

masq.COPING.for.merging <- masq.COPING.reordered %>%
  mutate(sample = "COPING") %>%
  relocate(sample, .after = ID)

masq.merged <- full_join(masq.RAMP.for.merging,masq.COPING.for.merging)

dim(masq.RAMP.for.merging)
dim(masq.COPING.for.merging)
dim(masq.merged)
names(masq.merged)
```

## drop data from waves after 6 April 2021 (see pre registration for re-agreed data boundaries)

```{r drop later waves}
masq.clean <- masq.merged %>%
  select(!contains("Wave_18"),
         -endDate,
         -startDate)

names(masq.clean)



```

# create sum scores

## recode variables so that it is reversed (higher scores are negatve) and range 0-4 instead of 1-5

also make -77 NA. This will mean we can no longer distinguish between missing variable and someone who is missing altogether(did not complete survey) but this should be evident from the number of peope whos NA == 7 per wave.

This will take this from being a score where higher == positive affect to higher ==  negative affect
```{r recode MASQ}

masq.recode <- masq.clean %>%
  mutate_at(vars(starts_with("masq")),
            funs(case_when(
             . == 1 ~ 4,
             . == 2 ~ 3,
             . == 3 ~ 2,
             . == 4 ~ 1,
             . == 5 ~ 0,
             . == -77 ~ NA_real_,
             TRUE ~  NA_real_)))
  
masq.recode %>%
  na_if(-77) %>%
  group_by(sample) %>%
    summarise(mean= mean(masq.pandemic_felt_optimistic_.Wave_03, na.rm = T) )  
```

check items before and then after recoding numbers for randomly selected items to make sure the direction is correct

```{r check recode before}

masq.clean %>%
  group_by(sample) %>%
  freq(masq.pandemic_lot_felt_energy_.Wave_0) 

## check some item averages from baseline to compare against qualtrics output
masq.clean %>%
  na_if(-77) %>%
  group_by(sample) %>%
  summarise(mean= mean(masq.pandemic_lot_felt_energy_.Wave_03, na.rm = T) )

masq.clean %>%
  na_if(-77) %>%
  group_by(sample) %>%
    summarise(mean= mean(masq.pandemic_felt_like_i_accomplished_a_lot_.Wave_03, na.rm = T) )

masq.clean %>%
  na_if(-77) %>%
  group_by(sample) %>%
    summarise(mean= mean(masq.pandemic_felt_optimistic_.Wave_03, na.rm = T) )

```

after recode numbers for randomly selected item
```{r check recode after}

masq.recode %>%
  group_by(sample) %>%
  freq(masq.pandemic_lot_felt_energy_.Wave_03) 

## check some item averages from baseline to compare against qualtrics 

masq.recode %>%
  group_by(sample) %>%
  summarise(mean= mean(masq.pandemic_lot_felt_energy_.Wave_0, na.rm = T) )

masq.recode %>%
  group_by(sample) %>%
    summarise(mean= mean(masq.pandemic_felt_like_i_accomplished_a_lot_.Wave_03, na.rm = T) )

masq.recode %>%
  group_by(sample) %>%
    summarise(mean= mean(masq.pandemic_felt_optimistic_.Wave_03, na.rm = T) )

```

## create total score parameter info

scoring variables to use in function below
```{r}
#Scoring variables
MASQ.n.items = 10 # Enter here the total number of items of the questionnaire
MASQ.maximum.missing.items = 2 # Enter here the number of required items a participant can miss before they are dropped

#Limits for data cleaning
MASQ.total.score_upper_limit = 40
MASQ.total.score_lower_limit = 0

MASQ.min.value = 0
MASQ.max.value = 4
```


## add columns for sum scores for each wave. {.tabset}
use functions that generate items for masq using wave number as a user input. This function is specific to my data naming conventions.

Use a second function to calculate the total scores. This can be applied to any scale. it takes a dataframe, list of keys, list of items, minimum and maximum item values, and maximum allowed missing items. It calculates total scores, using mean imputation for any missing items. It drops scores for anyone who misses more than the maximum allowed items. It describes how many missing items there are, and the total scores after dropping anyone for high missingness, and shows the internal consistency metrics for the scale. Finally, it returns  column of total scores as output.


***Will do a chunk for every wave to make output clear and easy to distinguish and examine for every wave***

### Baseline 
```{r baseline sumscore test}

keys_masq <- c(1,1,1,1,1,1,1,1,1,1)
items_masq <- generate_items_masq(0)

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_0 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 01
```{r}


items_masq <- generate_items_masq("01")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_01 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 02
```{r}


items_masq <- generate_items_masq("02")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_02 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 03
```{r}


items_masq <- generate_items_masq("03")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_03 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 04
```{r}


items_masq <- generate_items_masq("04")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_04 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 05
```{r}


items_masq <- generate_items_masq("05")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_05 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 06
```{r}


items_masq <- generate_items_masq("06")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_06 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 07
```{r}


items_masq <- generate_items_masq("07")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_07 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 08
```{r}


items_masq <- generate_items_masq("08")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_08 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 09
```{r}


items_masq <- generate_items_masq("09")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_09 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 10
```{r}


items_masq <- generate_items_masq("10")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_10 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 11
```{r}


items_masq <- generate_items_masq("11")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_11 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 12
```{r}


items_masq <- generate_items_masq("12")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_12 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 13
```{r}


items_masq <- generate_items_masq("13")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_13 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 14
```{r}


items_masq <- generate_items_masq("14")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_14 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 15
```{r}


items_masq <- generate_items_masq("15")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_15 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 16
```{r}


items_masq <- generate_items_masq("16")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_16 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

### Follow up 17
```{r}


items_masq <- generate_items_masq("17")

masq.recode <- masq.recode %>%
  mutate(masq.total_Wave_17 = calculate_totals(dataframe=.,
                                                itemlist = items_masq,
                                                keylist = keys_masq,
                                                minval = MASQ.min.value,
                                                maxval = MASQ.max.value,
                                                maxmissing = MASQ.maximum.missing.items))
```

# select total scores per wave and save this dataset

```{r drop item data}

masq.totals.clean <-
  masq.recode %>%
  select(ID, sample,
         starts_with("masq.total_Wave_"))
```


## save
```{r save final data}
saveRDS(object = masq.totals.clean, file = paste0("../../../data_clean/masq/masq.clean_merged_total_scores",  ".rds"))
```


## save as csv without any rown names for MPLUS

create a list of column names in order to save alongside the data file 
```{r mplus remove id make name codebook}

masq.mplus <- masq.totals.clean %>%
  select(-sample)

masq.mplus.names <- names(masq.mplus)
masq.mplus.names.columns <- seq(1,length(masq.mplus.names))

masq.mplus.names <- data.frame(cbind(masq.mplus.names,masq.mplus.names.columns))

names(masq.mplus.names) <- c("Variable","Column")



masq.mplus[is.na(masq.mplus)] <- -99

```

## make a new hased ID column with only numbers and save the linking file for later steps
MPlus will only allow numeric variables. 

hashed ID is already totally anonymised and disconnected from original dataset, so will save it with the other data as there is no additional risk of data linkage by doing so.

```{r numeric hashed id}
# create a random integer ID for each row, without duplication
masq.mplus$hash_id <- sample(1:100000,dim(masq.mplus)[1],replace=FALSE)

```


### create a linked id file

```{r linked ids}

masq.id.lnk <- masq.mplus %>%
  select(ID, hash_id)

```

### remove original ID and move hash id to the front
```{r remove ID relocate hash id}

masq.mplus <- masq.mplus %>%
  relocate(hash_id, .before = ID) %>%
  select(!ID)
```

## Drop timepoints with zero covariance covarage (no overlap between time points)

due to RAMP or COPING not having data 

Also create a keyed name dictionary

```{r mplus revised data}

masq.mplus.reduced <-masq.mplus %>%
  select(-masq.total_Wave_01,
         -masq.total_Wave_02)

masq.mplus.reduced.names <- names(masq.mplus.reduced)
masq.mplus.reduced.names.columns <- seq(1,length(masq.mplus.reduced.names))
masq.mplus.reduced.names <- data.frame(cbind(masq.mplus.reduced.names,masq.mplus.reduced.names.columns))

```

## remove column names
```{r remove col names}

names(masq.mplus) <- NULL
names(masq.mplus.reduced) <- NULL

```

##save .dat file for MPlus without headers, and corresponding orderd list of variables

```{r save plus dat files}

write.table(masq.mplus, "../../../data_clean/masq/masq.clean_merged_total_scores.csv",row.names = FALSE)
write.table(masq.mplus.reduced, "../../../data_clean/masq/masq.clean_merged_total_scores_reduced.csv",row.names = FALSE)
write_csv(masq.id.lnk, "../../../data_clean/masq/masq.mplus_hash_id_link_file.csv")
write_csv(masq.mplus.names, "../../../data_clean/masq/masq.clean_merged_total_scores_codebook.csv")
write_csv(masq.mplus.reduced.names, "../../../data_clean/masq/masq.clean_merged_total_scores_reduced_codebook.csv")

```


# summary table 

## create a  summary table for every wave

each component table by wave
```{r built summary table 0}

summary_table_0 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_0,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_0,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_0)))  %>%
    mutate(sample = "Combined",
      time.point = "Baseline") %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_0 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_0,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_0,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_0)))  %>%
    mutate(time.point = "Baseline" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_0 <- rbind (summary_table_sample_0,summary_table_0)

```



```{r built summary table 01}

summary_table_01 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_01,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_01,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_01)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 01" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_01 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_01,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_01,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_01)))  %>%
    mutate(time.point = "Follow up 01" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_01 <- rbind (summary_table_sample_01,summary_table_01)

```


```{r built summary table 02}

summary_table_02 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_02,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_02,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_02)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 02" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_02 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_02,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_02,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_02)))  %>%
    mutate(time.point = "Follow up 02" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_02 <- rbind (summary_table_sample_02,summary_table_02)

```

```{r built summary table 03}

summary_table_03 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_03,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_03,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_03)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 03" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_03 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_03,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_03,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_03)))  %>%
    mutate(time.point = "Follow up 03" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_03 <- rbind (summary_table_sample_03,summary_table_03)

```

```{r built summary table 04}

summary_table_04 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_04,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_04,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_04)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 04" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_04 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_04,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_04,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_04)))  %>%
    mutate(time.point = "Follow up 04" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_04 <- rbind (summary_table_sample_04,summary_table_04)

```

```{r built summary table 05}

summary_table_05 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_05,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_05,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_05)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 05" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_05 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_05,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_05,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_05)))  %>%
    mutate(time.point = "Follow up 05" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_05 <- rbind (summary_table_sample_05,summary_table_05)

```

```{r built summary table 06}

summary_table_06 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_06,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_06,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_06)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 06" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_06 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_06,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_06,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_06)))  %>%
    mutate(time.point = "Follow up 06" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_06 <- rbind (summary_table_sample_06,summary_table_06)

```

```{r built summary table 07}

summary_table_07 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_07,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_07,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_07)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 07" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_07 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_07,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_07,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_07)))  %>%
    mutate(time.point = "Follow up 07" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_07 <- rbind (summary_table_sample_07,summary_table_07)

```

```{r built summary table 08}

summary_table_08 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_08,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_08,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_08)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 08" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_08 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_08,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_08,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_08)))  %>%
    mutate(time.point = "Follow up 08" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_08 <- rbind (summary_table_sample_08,summary_table_08)

```

```{r built summary table 09}

summary_table_09 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_09,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_09,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_09)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 09" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_09 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_09,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_09,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_09)))  %>%
    mutate(time.point = "Follow up 09" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_09 <- rbind (summary_table_sample_09,summary_table_09)

```

```{r built summary table 10}

summary_table_10 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_10,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_10,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_10)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 10" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_10 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_10,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_10,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_10)))  %>%
    mutate(time.point = "Follow up 10" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_10 <- rbind (summary_table_sample_10,summary_table_10)

```

```{r built summary table 11}

summary_table_11 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_11,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_11,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_11)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 11" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_11 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_11,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_11,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_11)))  %>%
    mutate(time.point = "Follow up 11" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_11 <- rbind (summary_table_sample_11,summary_table_11)

```

```{r built summary table 12}

summary_table_12 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_12,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_12,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_12)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 12" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_12 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_12,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_12,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_12)))  %>%
    mutate(time.point = "Follow up 12" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_12 <- rbind (summary_table_sample_12,summary_table_12)

```

```{r built summary table 13}

summary_table_13 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_13,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_13,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_13)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 13" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_13 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_13,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_13,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_13)))  %>%
    mutate(time.point = "Follow up 13" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_13 <- rbind (summary_table_sample_13,summary_table_13)

```

```{r built summary table 14}

summary_table_14 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_14,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_14,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_14)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 14" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_14 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_14,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_14,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_14)))  %>%
    mutate(time.point = "Follow up 14" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_14 <- rbind (summary_table_sample_14,summary_table_14)

```

```{r built summary table 15}

summary_table_15 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_15,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_15,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_15)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 15" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_15 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_15,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_15,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_15)))  %>%
    mutate(time.point = "Follow up 15" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_15 <- rbind (summary_table_sample_15,summary_table_15)

```

```{r built summary table 16}

summary_table_16 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_16,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_16,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_16)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 16" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_16 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_16,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_16,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_16)))  %>%
    mutate(time.point = "Follow up 16" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_16 <- rbind (summary_table_sample_16,summary_table_16)

```

```{r built summary table 17}

summary_table_17 <- 
  
  masq.totals.clean %>%
  summarise(mean = mean(masq.total_Wave_17,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_17,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_17)))  %>%
    mutate(sample = "Combined",
      time.point = "Follow up 17" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)

summary_table_sample_17 <- 
  
  masq.totals.clean %>%
  group_by(sample) %>%
  summarise(mean = mean(masq.total_Wave_17,na.rm = TRUE),
            standard.deviation = sd(masq.total_Wave_17,na.rm = TRUE),
            valid.data.points =  sum(!is.na(masq.total_Wave_17)))  %>%
    mutate(time.point = "Follow up 17" ) %>%
  relocate(time.point,sample,valid.data.points, "mean", standard.deviation)
  

table_time_17 <- rbind (summary_table_sample_17,summary_table_17)

```

## join all tables


```{r join all waves}

full_table <- rbind(table_time_0,table_time_01,table_time_02,table_time_03,table_time_04,table_time_05,
                    table_time_06,table_time_07,table_time_08,table_time_09,table_time_10,
                    table_time_11,table_time_12,table_time_13,table_time_14,table_time_15,
                    table_time_16,table_time_17)

print(full_table)
```

## prettify and save with KABLE (colour combined rows)

```{r create an index list to specify coloured rows}
colour.me <- which(full_table$sample == "Combined")
```

```{r make kable table}

pretty_table <- full_table %>%
  kable(booktabs = T,
        align='llccc',
        digits = 2,
        col.names = str_to_title(gsub("[.]", " ", names(full_table)))) %>%
  kable_styling() %>%
  row_spec(colour.me,bold=T,background = "lightgrey") %>%
  row_spec(0,bold=T,background = "grey",font_size = 16)

pretty_table 
```
```{r save kable}

save_file <- file.path(dirname(dirname(getwd())),"output/outcome_variable_summary_tables/masq.all_waves_summary_table.html")

save_kable(pretty_table,
           save_file)

```




# examine data (total scores) for normality

## descriptives


```{r}

### Check distributions of raw data, square root transformed data and log transformed data (bring above 1 first...) using histograms

masq.totals.clean %>%
  describe(.)

```

##  histograms

```{r differentials_variable_transformation_histograms,fig.height=12,fig.width=8}

layout(matrix(c(1:18), nrow=6, byrow=T))

hist(masq.totals.clean$masq.total_Wave_0, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_01, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_02, breaks="FD")

hist(masq.totals.clean$masq.total_Wave_03, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_04, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_05, breaks="FD")

hist(masq.totals.clean$masq.total_Wave_06, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_07, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_08, breaks="FD")

hist(masq.totals.clean$masq.total_Wave_09, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_10, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_11, breaks="FD")

hist(masq.totals.clean$masq.total_Wave_12, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_13, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_14, breaks="FD")

hist(masq.totals.clean$masq.total_Wave_15, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_16, breaks="FD")
hist(masq.totals.clean$masq.total_Wave_17, breaks="FD")

layout(1)


```
# Internal consistency

```{r calculate internal consistency}

RAMP_baseline <- read_rds("/Users/katherineyoung/Documents/RAMP-COPING-longitudinal_common/data_clean/masq/masq.clean_baseline_items.RAMP.rds")


RAMP_baseline <- RAMP_baseline %>% 
  dplyr::rename(
    ID = ID,
    masq.pandemic_felt_successful= masq.pandemic_felt_successful_.Wave_0,
    masq.pandemic_felt_really_happy = masq.pandemic_felt_really_happy_.Wave_0,
    masq.pandemic_felt_optimistic = masq.pandemic_felt_optimistic_.Wave_0,
    masq.pandemic_lot_felt_fun = masq.pandemic_lot_felt_fun_.Wave_0, 
    masq.pandemic_felt_like_i_accomplished_a_lot = masq.pandemic_felt_like_i_accomplished_a_lot_.Wave_0,
    masq.pandemic_forward_lot_felt = masq.pandemic_forward_lot_felt_.Wave_0,
    masq.pandemic_felt_really_talkative = masq.pandemic_felt_really_talkative_.Wave_0, 
    masq.pandemic_felt_really_up_or_lively = masq.pandemic_felt_really_up_or_lively_.Wave_0,
    masq.pandemic_lot_felt_energy = masq.pandemic_lot_felt_energy_.Wave_0,
    masq.pandemic_felt_really_good_about_myself = masq.pandemic_felt_really_good_about_myself_.Wave_0)

RAMP_baseline <- RAMP_baseline[,c(1,4:13)]
COPING_followupA.id <- COPING_followupA.id[,c(1,5:14)]

merged_baseline <- rbind(RAMP_baseline, COPING_followupA.id)

psych::alpha(merged_baseline[,c(2:11)])
psych::alpha(merged_baseline[,c(2:11)])$total$std.alpha

```


